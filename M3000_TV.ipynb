{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_available_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Number of available CPUs:\", num_available_cpus)\n",
    "\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../new_flows\")\n",
    "from flows import RealNVP, Planar, MAF\n",
    "from models import NormalizingFlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.flows.autoregressive import MaskedAutoregressiveFlow\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform, MaskedPiecewiseQuadraticAutoregressiveTransform, MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process bkg samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1\n",
    "\n",
    "Mjj_cut = 1200\n",
    "pt_cut = 550\n",
    "eta_cut = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device =\", device)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor') if torch.cuda.is_available() else print ('cpu')\n",
    "\n",
    "torch.set_num_threads(num_available_cpus)\n",
    "\n",
    "print(\"Number of threads:\", torch.get_num_threads())\n",
    "print(\"Number of interop threads:\", torch.get_num_interop_threads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_data, bkg_unnorm_data, bkg_masses = LAPS_train(sample_type = 'qcdbkg', num_batches = num_batches)\n",
    "num_bkg_samples = bkg_data.shape[0]\n",
    "\n",
    "bkg_mean = np.mean(bkg_unnorm_data, axis=0)\n",
    "bkg_std = np.std(bkg_unnorm_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_bkg_samples)\n",
    "\n",
    "plt.hist(bkg_masses, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureBkg = torch.tensor(bkg_data)\n",
    "total_PureBkg_selection = total_PureBkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureBkg_selection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10000 * num_batches\n",
    "bkgAE_train_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs, shuffle=True) \n",
    "bkgAE_test_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the bkg-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 14\n",
    "hidden_features = 56\n",
    "\n",
    "num_layers = 4\n",
    "num_blocks_per_layer = 4\n",
    "#num_iter = 10000\n",
    "num_iter = 1000\n",
    "print_interval = 20\n",
    "\n",
    "#Current flow_type options: 'MAF', 'NSQUAD' (neural spline quadratic), 'NSRATQUAD' (neural spline rational quadratic)\n",
    "flow_type = 'NSQUAD'\n",
    "\n",
    "save_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device =\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(bkg_data[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_loss_dict = dict()\n",
    "bkg_flow_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Pure_NF_%s_k%s_hf%s_nbpl%s' % (flow_type, num_layers, hidden_features, num_blocks_per_layer)\n",
    "\n",
    "print(\"FCNN Hidden Layer Width: \", hidden_features)\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "bkg_base_dist = StandardNormal(shape=[num_features])\n",
    "\n",
    "bkg_transforms = []\n",
    "for _ in range(num_layers):\n",
    "    bkg_transforms.append(ReversePermutation(features=num_features))\n",
    "    if flow_type == 'MAF': \n",
    "        bkg_transforms.append(MaskedAffineAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features))\n",
    "    elif flow_type == 'NSQUAD': \n",
    "        bkg_transforms.append(MaskedPiecewiseQuadraticAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "    elif flow_type == 'NSRATQUAD': \n",
    "        bkg_transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "\n",
    "bkg_transform = CompositeTransform(bkg_transforms)\n",
    "\n",
    "bkg_flow = Flow(bkg_transform, bkg_base_dist)\n",
    "#print(bkg_flow)\n",
    "\n",
    "bkg_optimizer = optim.Adam(bkg_flow.parameters())\n",
    "\n",
    "bkg_tick = time.time()\n",
    "\n",
    "bkg_min_loss = 999999\n",
    "bkg_best_flow = None\n",
    "\n",
    "cur_losses = []\n",
    "patience_count = 0\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    terminate = False\n",
    "\n",
    "    for batch_idx, x in enumerate(bkgAE_train_iterator):\n",
    "\n",
    "        #x, y = datasets.make_moons(1024, noise=.1)\n",
    "        #x = bkg_tr_data\n",
    "        #x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        bkg_optimizer.zero_grad()\n",
    "        loss = -bkg_flow.log_prob(inputs=x)[0].mean()\n",
    "        loss.backward()\n",
    "        bkg_optimizer.step()\n",
    "\n",
    "        if batch_idx == len(bkgAE_train_iterator) - 1 :\n",
    "\n",
    "            xline = torch.linspace(-2, 2)\n",
    "            yline = torch.linspace(-2, 2)\n",
    "            xgrid, ygrid = torch.meshgrid(xline, yline)\n",
    "            xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #bkg_zgrid = bkg_flow.log_prob(xyinput)[0].exp().reshape(100, 100)\n",
    "                bkg_zgrid = -bkg_flow.log_prob(x)[0].cpu().numpy()\n",
    "\n",
    "                #print(bkg_zgrid.shape)\n",
    "                #bkg_zgrid = bkg_zgrid[bkg_zgrid < 10]\n",
    "                #print(bkg_zgrid.shape)\n",
    "                #print(bkg_zgrid[:5])\n",
    "\n",
    "            if (i + 1) % print_interval == 0: \n",
    "                print('bkg Iteration {} Complete'.format(i + 1))\n",
    "\n",
    "            bkg_print_loss = loss.detach().cpu().numpy()\n",
    "            cur_losses.append(bkg_print_loss) \n",
    "            print('Loss: ', bkg_print_loss)\n",
    "\n",
    "            if bkg_print_loss < bkg_min_loss: \n",
    "                patience_count = 0\n",
    "                bkg_best_flow = bkg_flow\n",
    "                if save_models: \n",
    "                    torch.save(bkg_flow, \"new_sample_flows/M3000_TV/bkg_%s.pt\" % (filename))\n",
    "                bkg_min_loss = bkg_print_loss\n",
    "                if (i + 1) % print_interval == 0: \n",
    "                    print('SAVING MODEL')\n",
    "            else: \n",
    "                patience_count += 1\n",
    "                if (i + 1) % print_interval == 0: \n",
    "                    print('NOT SAVING MODEL (PATIENCE = %s)' % patience_count)\n",
    "                if patience_count == 10: \n",
    "                    terminate = True\n",
    "                    break\n",
    "\n",
    "            bkg_tock = time.time()\n",
    "\n",
    "            if (i + 1) % print_interval == 0: \n",
    "                print('Time: ', bkg_tock - bkg_tick)\n",
    "                print('------------------------------------')\n",
    "                !nvidia-smi\n",
    "\n",
    "            #plt.contourf(xgrid.numpy(), ygrid.numpy(), bkg_zgrid.numpy())\n",
    "            #plt.title('iteration {}'.format(i + 1))\n",
    "            #plt.show()\n",
    "\n",
    "            #plt.hist(bkg_zgrid, bins=30, histtype='step')\n",
    "            #plt.title('iteration {}'.format(i + 1))\n",
    "            #plt.show()\n",
    "            \n",
    "    if terminate: \n",
    "        break\n",
    "\n",
    "bkg_loss_dict[hidden_features] = float(bkg_min_loss)\n",
    "bkg_flow_list.append(bkg_best_flow)\n",
    "\n",
    "print('------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cur_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Bkg-trained loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process sig samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1\n",
    "\n",
    "Mjj_cut = 1200\n",
    "pt_cut = 550\n",
    "eta_cut = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_data, sig_unnorm_data, sig_masses = LAPS_train(sample_type = 'Wp3000', num_batches = num_batches, inp_meanstd = (bkg_mean, bkg_std))\n",
    "num_sig_samples = sig_data.shape[0]\n",
    "\n",
    "sig_training_data_percentage = 98\n",
    "num_sig_training_samples = int(sig_training_data_percentage * num_sig_samples / 100)\n",
    "\n",
    "indices = np.random.permutation(num_sig_samples)\n",
    "sig_training_indices, sig_testing_indices = indices[:num_sig_training_samples], indices[num_sig_training_samples:]\n",
    "sig_training_data, sig_testing_data = sig_data[sig_training_indices], sig_data[sig_testing_indices]\n",
    "sig_unnorm_training_data, sig_unnorm_testing_data = sig_unnorm_data[sig_training_indices], sig_unnorm_data[sig_testing_indices]\n",
    "sig_training_masses, sig_testing_masses = sig_masses[sig_training_indices], sig_masses[sig_testing_indices]\n",
    "\n",
    "print(num_sig_samples)\n",
    "print(num_sig_training_samples)\n",
    "print(sig_testing_data.shape[0])\n",
    "print(num_sig_training_samples + sig_testing_data.shape[0] == num_sig_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_sig_training_samples)\n",
    "\n",
    "plt.hist(sig_training_masses, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureSig = torch.tensor(sig_training_data)\n",
    "total_PureSig_selection = total_PureSig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureSig_selection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3500 * num_batches\n",
    "sigAE_train_iterator = utils.DataLoader(total_PureSig_selection, batch_size=bs, shuffle=True) \n",
    "sigAE_test_iterator = utils.DataLoader(total_PureSig_selection, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the sig-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 14\n",
    "hidden_features = 56\n",
    "\n",
    "num_layers = 4\n",
    "num_blocks_per_layer = 4\n",
    "#num_iter = 10000\n",
    "num_iter = 1000\n",
    "print_interval = 20\n",
    "\n",
    "#Current flow_type options: 'MAF', 'NSQUAD' (neural spline quadratic), 'NSRATQUAD' (neural spline rational quadratic)\n",
    "flow_type = 'NSQUAD'\n",
    "\n",
    "save_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device =\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sig_training_data[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_loss_dict = dict()\n",
    "sig_flow_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Pure_NF_%s_k%s_hf%s_nbpl%s' % (flow_type, num_layers, hidden_features, num_blocks_per_layer)\n",
    "\n",
    "print(\"FCNN Hidden Layer Width: \", hidden_features)\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "sig_base_dist = StandardNormal(shape=[num_features])\n",
    "\n",
    "sig_transforms = []\n",
    "for _ in range(num_layers):\n",
    "    sig_transforms.append(ReversePermutation(features=num_features))\n",
    "    if flow_type == 'MAF': \n",
    "        sig_transforms.append(MaskedAffineAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features))\n",
    "    elif flow_type == 'NSQUAD': \n",
    "        sig_transforms.append(MaskedPiecewiseQuadraticAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "    elif flow_type == 'NSRATQUAD': \n",
    "        sig_transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "\n",
    "sig_transform = CompositeTransform(sig_transforms)\n",
    "\n",
    "sig_flow = Flow(sig_transform, sig_base_dist)\n",
    "#print(sig_flow)\n",
    "\n",
    "sig_optimizer = optim.Adam(sig_flow.parameters())\n",
    "\n",
    "sig_tick = time.time()\n",
    "\n",
    "sig_min_loss = 999999\n",
    "sig_best_flow = None\n",
    "\n",
    "cur_losses = []\n",
    "patience_count = 0\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    terminate = False\n",
    "\n",
    "    for batch_idx, x in enumerate(sigAE_train_iterator):\n",
    "\n",
    "        #x, y = datasets.make_moons(1024, noise=.1)\n",
    "        #x = sig_tr_data\n",
    "        #x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        sig_optimizer.zero_grad()\n",
    "        loss = -sig_flow.log_prob(inputs=x)[0].mean()\n",
    "        loss.backward()\n",
    "        sig_optimizer.step()\n",
    "\n",
    "        if batch_idx == len(sigAE_train_iterator) - 1 :\n",
    "\n",
    "            xline = torch.linspace(-2, 2)\n",
    "            yline = torch.linspace(-2, 2)\n",
    "            xgrid, ygrid = torch.meshgrid(xline, yline)\n",
    "            xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #sig_zgrid = sig_flow.log_prob(xyinput)[0].exp().reshape(100, 100)\n",
    "                sig_zgrid = -sig_flow.log_prob(x)[0].cpu().numpy()\n",
    "\n",
    "                #print(sig_zgrid.shape)\n",
    "                #sig_zgrid = sig_zgrid[sig_zgrid < 10]\n",
    "                #print(sig_zgrid.shape)\n",
    "                #print(sig_zgrid[:5])\n",
    "                \n",
    "            if (i + 1) % print_interval == 0: \n",
    "                print('sig Iteration {} Complete'.format(i + 1))\n",
    "\n",
    "            sig_print_loss = loss.detach().cpu().numpy()\n",
    "            cur_losses.append(sig_print_loss) \n",
    "            print('Loss: ', sig_print_loss)\n",
    "\n",
    "            if sig_print_loss < sig_min_loss: \n",
    "                patience_count = 0\n",
    "                sig_best_flow = sig_flow\n",
    "                if save_models: \n",
    "                    torch.save(sig_flow, \"new_sample_flows/M3000_TV/sig_%s.pt\" % (filename))\n",
    "                sig_min_loss = sig_print_loss\n",
    "                if (i + 1) % print_interval == 0: \n",
    "                    print('SAVING MODEL')\n",
    "            else: \n",
    "                patience_count += 1\n",
    "                if (i + 1) % print_interval == 0: \n",
    "                    print('NOT SAVING MODEL (PATIENCE = %s)' % patience_count)\n",
    "                if patience_count == 10: \n",
    "                    terminate = True\n",
    "                    break\n",
    "\n",
    "            sig_tock = time.time()\n",
    "\n",
    "            if (i + 1) % print_interval == 0: \n",
    "                print('Time: ', sig_tock - sig_tick)\n",
    "                print('------------------------------------')\n",
    "                !nvidia-smi\n",
    "\n",
    "            #plt.contourf(xgrid.numpy(), ygrid.numpy(), sig_zgrid.numpy())\n",
    "            #plt.title('iteration {}'.format(i + 1))\n",
    "            #plt.show()\n",
    "\n",
    "            #plt.hist(sig_zgrid, bins=30, histtype='step')\n",
    "            #plt.title('iteration {}'.format(i + 1))\n",
    "            #plt.show()\n",
    "            \n",
    "    if terminate: \n",
    "        break\n",
    "\n",
    "sig_loss_dict[hidden_features] = float(sig_min_loss)\n",
    "sig_flow_list.append(sig_best_flow)\n",
    "\n",
    "print('------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cur_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Sig-trained Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bkg-trained post-training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_test = torch.tensor(bkg_data)\n",
    "sig_test = torch.tensor(sig_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_fpr_bkgtr(sigloss,bkgloss):\n",
    "    bins = np.linspace(0,100,10001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for cut in bins:\n",
    "        tpr.append(np.where(sigloss>cut)[0].shape[0]/len(sigloss))\n",
    "        fpr.append(np.where(bkgloss>cut)[0].shape[0]/len(bkgloss))\n",
    "\n",
    "    return tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bkg_flow_list = []\n",
    "\n",
    "new_bkg_loss_dict = dict()\n",
    "\n",
    "filename = 'Pure_NF_%s_k%s_hf%s_nbpl%s' % (flow_type, num_layers, hidden_features, num_blocks_per_layer)\n",
    "\n",
    "new_bkg_flow = torch.load(\"new_sample_flows/M3000_TV/bkg_%s.pt\" % (filename))\n",
    "\n",
    "new_bkg_flow_list.append(new_bkg_flow)\n",
    "\n",
    "new_bkg_loss = -new_bkg_flow.log_prob(bkg_data)[0].mean().detach().cpu().numpy()\n",
    "new_bkg_loss_dict[hidden_features] = new_bkg_loss\n",
    "\n",
    "print(bkg_loss_dict)\n",
    "print(new_bkg_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_tr = int(bkg_data.shape[0])\n",
    "\n",
    "with torch.no_grad(): \n",
    "    bkg_samples = new_bkg_flow_list[-1].sample(num_samples_tr).detach().cpu().numpy()\n",
    "#bkg_samples = new_bkg_flow_list[0].sample(num_samples_tr).detach().cpu().numpy()\n",
    "print(bkg_samples.shape)\n",
    "bkg_samples_bad_indices = np.argwhere((bkg_samples < -10) | (bkg_samples > 10))[:,0]\n",
    "print(bkg_samples_bad_indices[:25])\n",
    "new_bkg_samples = np.delete(bkg_samples, bkg_samples_bad_indices, axis = 0)\n",
    "\n",
    "print(new_bkg_samples.shape)\n",
    "\n",
    "print(\"Hidden Features: \", hidden_features)\n",
    "\n",
    "plot_titles = [r'$M_{j1}$', r'Jet 1 $\\tau_{21}$', r'Jet 1 $\\tau_{32}$', r'Jet 1 $\\tau_{43}$', r'Jet 1 $\\tau_s$', r'Jet 1 $P_b$', r'Jet 1 $n_{pf}$', \n",
    "              r'$M_{j2}$', r'Jet 2 $\\tau_{21}$', r'Jet 2 $\\tau_{32}$', r'Jet 2 $\\tau_{43}$', r'Jet 2 $\\tau_s$', r'Jet 2 $P_b$', r'Jet 2 $n_{pf}$',]\n",
    "\n",
    "for index in range(num_features): \n",
    "    n, bins, patches = plt.hist(bkg_data[:, index], bins=50, histtype='step', label='Input distribution')\n",
    "    plt.hist(new_bkg_samples[:, index], bins=bins, histtype='step', label='NF estimated density')\n",
    "    if index % 7 == 4: \n",
    "        plt.legend(loc=(1.04,0.85))\n",
    "    plt.title(plot_titles[index])\n",
    "    plt.show()\n",
    "    \n",
    "#plt.hist outputs binning, pass that as input to make binning the same for two hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_bkgloss = -new_bkg_flow.log_prob(bkg_data)[0].detach().cpu().numpy()\n",
    "bkgtr_sigloss = -new_bkg_flow.log_prob(sig_training_data)[0].detach().cpu().numpy()\n",
    "print(bkgtr_bkgloss.shape)\n",
    "print(bkgtr_sigloss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr = get_tpr_fpr_bkgtr(bkgtr_sigloss, bkgtr_bkgloss)\n",
    "tpr_np, fpr_np = np.array(tpr), np.array(fpr)\n",
    "\n",
    "nonzero_idx = np.nonzero(fpr_np)\n",
    "\n",
    "tpr_inverse = tpr_np[nonzero_idx]\n",
    "fpr_inverse = 1/fpr_np[nonzero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tpr_inverse,fpr_inverse)\n",
    "plt.xlabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "plt.ylabel(r'$1/\\epsilon_{bkg}$',fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.title('Bkg-trained Pure NF Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_auc = metrics.auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel(r'$\\epsilon_{bkg}$',fontsize=15)\n",
    "plt.ylabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "plt.title('Bkg-trained Pure NF Model (AUC = %s)' % round(bkgtr_auc,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sig-trained post-training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_fpr_sigtr(sigloss,bkgloss):\n",
    "    bins = np.linspace(0,100,10001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for cut in bins:\n",
    "        tpr.append(np.where(sigloss<cut)[0].shape[0]/len(sigloss))\n",
    "        fpr.append(np.where(bkgloss<cut)[0].shape[0]/len(bkgloss))\n",
    "\n",
    "    return tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sig_flow_list = []\n",
    "\n",
    "new_sig_loss_dict = dict()\n",
    "\n",
    "filename = 'Pure_NF_%s_k%s_hf%s_nbpl%s' % (flow_type, num_layers, hidden_features, num_blocks_per_layer)\n",
    "\n",
    "new_sig_flow = torch.load(\"new_sample_flows/M3000_TV/sig_%s.pt\" % (filename))\n",
    "\n",
    "new_sig_flow_list.append(new_sig_flow)\n",
    "\n",
    "new_sig_loss = -new_sig_flow.log_prob(sig_training_data)[0].mean().detach().cpu().numpy()\n",
    "new_sig_loss_dict[hidden_features] = new_sig_loss\n",
    "\n",
    "print(sig_loss_dict)\n",
    "print(new_sig_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_tr = int(sig_training_data.shape[0])\n",
    "\n",
    "with torch.no_grad(): \n",
    "    sig_samples = new_sig_flow_list[-1].sample(num_samples_tr).detach().cpu().numpy()\n",
    "#sig_samples = new_sig_flow_list[0].sample(num_samples_tr).detach().cpu().numpy()\n",
    "print(sig_samples.shape)\n",
    "sig_samples_bad_indices = np.argwhere((sig_samples < -10) | (sig_samples > 10))[:,0]\n",
    "print(sig_samples_bad_indices[:25])\n",
    "new_sig_samples = np.delete(sig_samples, sig_samples_bad_indices, axis = 0)\n",
    "\n",
    "print(new_sig_samples.shape)\n",
    "\n",
    "print(\"Hidden Features: \", hidden_features)\n",
    "\n",
    "plot_titles = [r'$M_{j1}$', r'Jet 1 $\\tau_{21}$', r'Jet 1 $\\tau_{32}$', r'Jet 1 $\\tau_{43}$', r'Jet 1 $\\tau_s$', r'Jet 1 $P_b$', r'Jet 1 $n_{pf}$', \n",
    "              r'$M_{j2}$', r'Jet 2 $\\tau_{21}$', r'Jet 2 $\\tau_{32}$', r'Jet 2 $\\tau_{43}$', r'Jet 2 $\\tau_s$', r'Jet 2 $P_b$', r'Jet 2 $n_{pf}$',]\n",
    "\n",
    "for index in range(num_features): \n",
    "    n, bins, patches = plt.hist(sig_training_data[:, index], bins=50, histtype='step', label='Input distribution')\n",
    "    plt.hist(new_sig_samples[:, index], bins=bins, histtype='step', label='NF estimated density')\n",
    "    if index % 7 == 4: \n",
    "        plt.legend(loc=(1.04,0.85))\n",
    "    plt.title(plot_titles[index])\n",
    "    plt.show()\n",
    "    \n",
    "#plt.hist outputs binning, pass that as input to make binning the same for two hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_bkgloss = -new_sig_flow.log_prob(bkg_data)[0].detach().cpu().numpy()\n",
    "sigtr_sigloss = -new_sig_flow.log_prob(sig_training_data)[0].detach().cpu().numpy()\n",
    "print(sigtr_bkgloss.shape)\n",
    "print(sigtr_sigloss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr = get_tpr_fpr_sigtr(sigtr_sigloss, sigtr_bkgloss)\n",
    "tpr_np, fpr_np = np.array(tpr), np.array(fpr)\n",
    "\n",
    "nonzero_idx = np.nonzero(fpr_np)\n",
    "\n",
    "tpr_inverse = tpr_np[nonzero_idx]\n",
    "fpr_inverse = 1/fpr_np[nonzero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tpr_inverse,fpr_inverse)\n",
    "plt.xlabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "plt.ylabel(r'$1/\\epsilon_{bkg}$',fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.title('Sig-trained Pure NF Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_auc = metrics.auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel(r'$\\epsilon_{bkg}$',fontsize=15)\n",
    "plt.ylabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "plt.title('Sig-trained Pure NF Model (AUC = %s)' % round(sigtr_auc,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bump Hunt CSV File Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bkg_batches = 2\n",
    "num_sig_batches = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_data, bkg_unnorm_data, bkg_masses = LAPS_test(sample_type = 'qcdbkg', num_batches = num_bkg_batches)\n",
    "\n",
    "bkg_mean = np.mean(bkg_unnorm_data, axis=0)\n",
    "bkg_std = np.std(bkg_unnorm_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bkg_samples = bkg_data.shape[0]\n",
    "num_sig_samples = sig_testing_data.shape[0]\n",
    "sig_sample_percentage = 100 * num_sig_samples / (num_bkg_samples + num_sig_samples)\n",
    "\n",
    "bkg_data_WL = np.concatenate((bkg_data, np.zeros((num_bkg_samples,1), dtype='float32')), axis=1)\n",
    "sig_testing_data_WL = np.concatenate((sig_testing_data, np.ones((num_sig_samples,1), dtype='float32')), axis=1)\n",
    "\n",
    "test_data = np.concatenate((bkg_data_WL, sig_testing_data_WL), axis=0)\n",
    "test_masses = np.concatenate((bkg_masses, sig_testing_masses), axis=0)\n",
    "#test_data = bkg_data_WL\n",
    "#test_masses = bkg_masses\n",
    "test_data_WM = np.concatenate((test_masses, test_data), axis=1)\n",
    "\n",
    "np.random.shuffle(test_data_WM)\n",
    "\n",
    "test_masses = test_data_WM[:,0]\n",
    "test_labels = test_data_WM[:,-1]\n",
    "test_data = test_data_WM[:,1:-1]\n",
    "\n",
    "num_test_samples = test_data.shape[0]\n",
    "\n",
    "print(\"Num bkg samples: \", num_bkg_samples)\n",
    "print(\"Num sig samples: \", num_sig_samples)\n",
    "print(\"Num test samples: \", num_test_samples)\n",
    "print(\"Signal Percentage: \", sig_sample_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mass = pd.DataFrame(np.ndarray.tolist(test_masses))\n",
    "df_mass.to_csv('csv_files/test_masses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Pure_NF_%s_k%s_hf%s_nbpl%s' % (flow_type, num_layers, hidden_features, num_blocks_per_layer)\n",
    "bkg_model = torch.load(\"new_sample_flows/M3000_TV/bkg_%s.pt\" % (filename))\n",
    "sig_model = torch.load(\"new_sample_flows/M3000_TV/sig_%s.pt\" % (filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_loss_indices = np.where(test_labels==0)\n",
    "sig_loss_indices = np.where(test_labels==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_test_losses = -bkg_model.log_prob(test_data)[0].detach().cpu().numpy()\n",
    "print(bkgtr_test_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_bkg_losses = bkgtr_test_losses[bkg_loss_indices]\n",
    "print(bkgtr_bkg_losses.shape)\n",
    "\n",
    "bkgtr_sig_losses = bkgtr_test_losses[sig_loss_indices]\n",
    "print(bkgtr_sig_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bkgloss = pd.DataFrame(np.ndarray.tolist(bkgtr_test_losses))\n",
    "df_bkgloss.to_csv('csv_files/bkgtr_test_losses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_test_losses = -sig_model.log_prob(test_data)[0].detach().cpu().numpy()\n",
    "print(sigtr_test_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_bkg_losses = sigtr_test_losses[bkg_loss_indices]\n",
    "print(sigtr_bkg_losses.shape)\n",
    "\n",
    "sigtr_sig_losses = sigtr_test_losses[sig_loss_indices]\n",
    "print(sigtr_sig_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sigloss = pd.DataFrame(np.ndarray.tolist(sigtr_test_losses))\n",
    "df_sigloss.to_csv('csv_files/sigtr_test_losses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgloss_cut = 15\n",
    "low_bkgloss_indices = np.where(bkgtr_test_losses > bkgloss_cut)[0]\n",
    "for index in range(2,8):  \n",
    "    \n",
    "    sigloss_cut = 5*index\n",
    "    low_sigloss_indices = np.where(sigtr_test_losses < sigloss_cut)[0]\n",
    "    low_loss_indices = np.intersect1d(low_bkgloss_indices, low_sigloss_indices)\n",
    "    low_loss_test_masses = test_masses[low_loss_indices]\n",
    "    low_loss_bkg_masses = test_masses[np.intersect1d(low_loss_indices, bkg_loss_indices)]\n",
    "    \n",
    "    n, bins, patches = plt.hist(low_loss_test_masses, bins=50, histtype = 'step', label = 'bkg + sig')\n",
    "    plt.hist(low_loss_bkg_masses, bins = bins, histtype = 'step', label = 'bkg only')\n",
    "    \n",
    "    plt.xlabel('bkgloss > %s, sigloss < %s' % (bkgloss_cut, sigloss_cut))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bkgtr_bkg_losses, sigtr_bkg_losses, s=2, label = 'Bkg samples')\n",
    "plt.scatter(bkgtr_sig_losses, sigtr_sig_losses, s=2, label = 'Sig samples')\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0,100)\n",
    "plt.xlabel(\"bkg-Trained Model Loss\")\n",
    "plt.ylabel(\"sig-Trained Model Loss\")\n",
    "plt.title(\"Testing Data QUAK Space\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process RSGraviton3000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSGraviton3000_data, _, _ = LAPS_test(sample_type = 'RSGraviton3000', num_batches = 1, inp_meanstd = (bkg_mean, bkg_std))\n",
    "num_RSGraviton3000_samples = RSGraviton3000_data.shape[0]\n",
    "\n",
    "RSGraviton3000_testing_data_percentage = 100 - sig_training_data_percentage\n",
    "num_RSGraviton3000_testing_samples = int(RSGraviton3000_testing_data_percentage * num_RSGraviton3000_samples / 100)\n",
    "print('Number of testing samples: %s' % num_RSGraviton3000_testing_samples)\n",
    "\n",
    "indices = np.random.permutation(num_RSGraviton3000_samples)\n",
    "RSGraviton3000_testing_indices = indices[:num_RSGraviton3000_testing_samples]\n",
    "RSGraviton3000_testing_data = RSGraviton3000_data[RSGraviton3000_testing_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_RSGraviton3000_losses = -bkg_model.log_prob(RSGraviton3000_testing_data)[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_RSGraviton3000_losses = -sig_model.log_prob(RSGraviton3000_testing_data)[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process Qstar3000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qstar3000_data, _, _ = LAPS_test(sample_type = 'Qstar3000', num_batches = 1, inp_meanstd = (bkg_mean, bkg_std))\n",
    "num_Qstar3000_samples = Qstar3000_data.shape[0]\n",
    "\n",
    "Qstar3000_testing_data_percentage = 100 - sig_training_data_percentage\n",
    "num_Qstar3000_testing_samples = int(Qstar3000_testing_data_percentage * num_Qstar3000_samples / 100)\n",
    "print('Number of testing samples: %s' % num_Qstar3000_testing_samples)\n",
    "\n",
    "indices = np.random.permutation(num_Qstar3000_samples)\n",
    "Qstar3000_testing_indices = indices[:num_Qstar3000_testing_samples]\n",
    "Qstar3000_testing_data = Qstar3000_data[Qstar3000_testing_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_Qstar3000_losses = -bkg_model.log_prob(Qstar3000_testing_data)[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_Qstar3000_losses = -sig_model.log_prob(Qstar3000_testing_data)[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process Wkk3000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wkk3000_data, _, _ = LAPS_test(sample_type = 'Wkk3000', num_batches = 1, inp_meanstd = (bkg_mean, bkg_std))\n",
    "num_Wkk3000_samples = Wkk3000_data.shape[0]\n",
    "\n",
    "Wkk3000_testing_data_percentage = 100 - sig_training_data_percentage\n",
    "num_Wkk3000_testing_samples = int(Wkk3000_testing_data_percentage * num_Wkk3000_samples / 100)\n",
    "print('Number of testing samples: %s' % num_Wkk3000_testing_samples)\n",
    "\n",
    "indices = np.random.permutation(num_Wkk3000_samples)\n",
    "Wkk3000_testing_indices = indices[:num_Wkk3000_testing_samples]\n",
    "Wkk3000_testing_data = Wkk3000_data[Wkk3000_testing_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_Wkk3000_losses = -bkg_model.log_prob(Wkk3000_testing_data)[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_Wkk3000_losses = -sig_model.log_prob(Wkk3000_testing_data)[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master QUAK Space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bad_loss_cutoff = 100\n",
    "y_bad_loss_cutoff = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_bkg_losses = np.append(bkgtr_bkg_losses, np.array([0,]))\n",
    "sigtr_bkg_losses = np.append(sigtr_bkg_losses, np.array([0,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bkgtr_bkg_losses, sigtr_bkg_losses, s=2, label = 'QCD bkg samples')\n",
    "plt.scatter(bkgtr_sig_losses, sigtr_sig_losses, s=2, label = r'''W'$\\rightarrow$tB' (M=3000) sig samples''')\n",
    "plt.scatter(bkgtr_Wkk3000_losses, sigtr_Wkk3000_losses, s=2, label = r'''Wkk$\\rightarrow$WR$\\rightarrow$W+WW (M=3000) sig samples''')\n",
    "plt.scatter(bkgtr_RSGraviton3000_losses, sigtr_RSGraviton3000_losses, s=2, label = r'''G$\\rightarrow$gg (M=3000) sig samples''')\n",
    "plt.scatter(bkgtr_Qstar3000_losses, sigtr_Qstar3000_losses, s=2, label = r'''q*$\\rightarrow$qW (M=3000) sig samples''')\n",
    "plt.xlim(0, x_bad_loss_cutoff)\n",
    "plt.ylim(0, y_bad_loss_cutoff)\n",
    "plt.xlabel('QCD Bkg Model Loss')\n",
    "plt.ylabel(r'''W'$\\rightarrow$tB' (M=3000) Sig Model Loss''')\n",
    "plt.title('Testing Data QUAK Space (Scatter Plot)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 5000\n",
    "\n",
    "bkgtr_all_losses = np.concatenate((bkgtr_bkg_losses, bkgtr_sig_losses, bkgtr_Wkk3000_losses, bkgtr_RSGraviton3000_losses, bkgtr_Qstar3000_losses))\n",
    "sigtr_all_losses = np.concatenate((sigtr_bkg_losses, sigtr_sig_losses, sigtr_Wkk3000_losses, sigtr_RSGraviton3000_losses, sigtr_Qstar3000_losses))\n",
    "\n",
    "plt.hist2d(bkgtr_all_losses, sigtr_all_losses, cmap = plt.cm.jet, bins=num_bins)\n",
    "plt.colorbar()\n",
    "plt.xlabel('QCD Bkg Model Loss')\n",
    "plt.ylabel(r'''W'$\\rightarrow$tB' (M=3000) Sig Model Loss''')\n",
    "plt.title('Testing Data QUAK Space (Heat Map)')\n",
    "plt.xlim(0, x_bad_loss_cutoff)\n",
    "plt.ylim(0, y_bad_loss_cutoff)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized Input Variable Density Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_titles = [r'$M_{j1}$', r'Jet 1 $\\tau_{21}$', r'Jet 1 $\\tau_{32}$', r'Jet 1 $\\tau_{43}$', r'Jet 1 $\\tau_s$', r'Jet 1 $P_b$', r'Jet 1 $n_{pf}$', \n",
    "              r'$M_{j2}$', r'Jet 2 $\\tau_{21}$', r'Jet 2 $\\tau_{32}$', r'Jet 2 $\\tau_{43}$', r'Jet 2 $\\tau_s$', r'Jet 2 $P_b$', r'Jet 2 $n_{pf}$',]\n",
    "\n",
    "for index in range(num_features): \n",
    "    n, bins, patches = plt.hist(bkg_data[:, index], bins=30, histtype='step', density=True, label='QCD bkg samples')\n",
    "    plt.hist(sig_testing_data[:, index], bins=bins, histtype='step', density=True, label=r'''W'$\\rightarrow$WZ (M=3000) sig samples''')\n",
    "    plt.hist(Wkk3000_data[:, index], bins=bins, histtype='step', density=True, label=r'''Wkk$\\rightarrow$WR$\\rightarrow$W+WW (M=3000) sig samples''')\n",
    "    plt.hist(RSGraviton3000_data[:, index], bins=bins, histtype='step', density=True, label=r'''G$\\rightarrow$gg (M=3000) sig samples''')\n",
    "    plt.hist(Qstar3000_data[:, index], bins=bins, histtype='step', density=True, label=r'''q*$\\rightarrow$qW (M=3000) sig samples''')\n",
    "    if index % 7 == 4: \n",
    "        plt.legend(loc=(1.04,0.64))\n",
    "    plt.title(plot_titles[index])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
