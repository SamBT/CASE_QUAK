{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_available_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Number of available CPUs:\", num_available_cpus)\n",
    "\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../new_flows\")\n",
    "from flows import RealNVP, Planar, MAF\n",
    "from models import NormalizingFlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.flows.autoregressive import MaskedAutoregressiveFlow\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform, MaskedPiecewiseQuadraticAutoregressiveTransform, MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1\n",
    "\n",
    "Mjj_cut = 1200\n",
    "pt_cut = 550\n",
    "eta_cut = None\n",
    "box_cox = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 6\n",
    "NS_hidden_features = 48\n",
    "flow_type = 'NSQUAD' #Options are 'MAF', 'Planar' (not recommended), 'NSQUAD', and 'NSRATQUAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device =\", device)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor') if torch.cuda.is_available() else print ('cpu')\n",
    "\n",
    "torch.set_num_threads(num_available_cpus)\n",
    "\n",
    "print(\"Number of threads:\", torch.get_num_threads())\n",
    "print(\"Number of interop threads:\", torch.get_num_interop_threads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_data = np.array([])\n",
    "\n",
    "for batch_number in range(num_batches): \n",
    "    train_batch = \"/nobackup/users/myunus/CASE_samples/BB_batch%s.h5\" % (batch_number)\n",
    "    f = h5py.File(train_batch, \"r\")\n",
    "    \n",
    "    if batch_number == 0: \n",
    "        print(\"Keys: %s\" % f.keys())\n",
    "    \n",
    "    jet_kinematics = f['jet_kinematics']\n",
    "    jet1_extraInfo = f['jet1_extraInfo']\n",
    "    jet2_extraInfo = f['jet2_extraInfo']\n",
    "    truth_label = f['truth_label']\n",
    "\n",
    "    np.seterr(invalid = 'ignore')\n",
    "\n",
    "    delta_eta = jet_kinematics[:,1]\n",
    "\n",
    "    Mjj = np.reshape(jet_kinematics[:,0], (-1,1))\n",
    "    Mj1 = np.reshape(jet_kinematics[:,5], (-1,1))\n",
    "    Mj2 = np.reshape(jet_kinematics[:,9], (-1,1))\n",
    "\n",
    "    jet1_pt = np.reshape(jet_kinematics[:,2], (-1,1))\n",
    "    jet2_pt = np.reshape(jet_kinematics[:,6], (-1,1))\n",
    "\n",
    "    jet1_tau1 = np.reshape(jet1_extraInfo[:,0], (-1,1))\n",
    "    jet1_tau2 = np.reshape(jet1_extraInfo[:,1], (-1,1))\n",
    "    jet1_tau3 = np.reshape(jet1_extraInfo[:,2], (-1,1))\n",
    "    jet1_tau4 = np.reshape(jet1_extraInfo[:,3], (-1,1))\n",
    "    #jet1_btagscore = np.reshape(jet1_extraInfo[:,5],(-1,1))\n",
    "    jet1_numpfconst = np.reshape(jet1_extraInfo[:,6],(-1,1))\n",
    "\n",
    "    jet1_tau21 = jet1_tau2 / jet1_tau1\n",
    "    jet1_tau32 = jet1_tau3 / jet1_tau2\n",
    "    jet1_tau43 = jet1_tau4 / jet1_tau3\n",
    "    jet1_sqrt_tau21 = np.sqrt(jet1_tau21) / jet1_tau1\n",
    "\n",
    "    jet2_tau1 = np.reshape(jet2_extraInfo[:,0], (-1,1))\n",
    "    jet2_tau2 = np.reshape(jet2_extraInfo[:,1], (-1,1))\n",
    "    jet2_tau3 = np.reshape(jet2_extraInfo[:,2], (-1,1))\n",
    "    jet2_tau4 = np.reshape(jet2_extraInfo[:,3], (-1,1))\n",
    "    #jet2_btagscore = np.reshape(jet2_extraInfo[:,5],(-1,1))\n",
    "    jet2_numpfconst = np.reshape(jet2_extraInfo[:,6],(-1,1))\n",
    "\n",
    "    jet2_tau21 = jet2_tau2 / jet2_tau1\n",
    "    jet2_tau32 = jet2_tau3 / jet2_tau2\n",
    "    jet2_tau43 = jet2_tau4 / jet2_tau3\n",
    "    jet2_sqrt_tau21 = np.sqrt(jet2_tau21) / jet2_tau1\n",
    "\n",
    "    truth_label = truth_label[:]\n",
    "    \n",
    "    data = np.concatenate((jet1_tau21, jet1_tau32, jet1_tau43, jet2_tau21, jet2_tau32, jet2_tau43), axis=1)\n",
    "\n",
    "    bkg_indices = np.where((truth_label == 0) \n",
    "                              & (Mjj > Mjj_cut) \n",
    "                              & (Mj1 > 0)\n",
    "                              & (Mj2 > 0)\n",
    "                              & (jet1_pt > pt_cut) \n",
    "                              & (jet2_pt > pt_cut)\n",
    "                              & (np.isfinite(jet1_tau21))\n",
    "                              & (np.isfinite(jet1_tau32))\n",
    "                              & (np.isfinite(jet1_tau43))\n",
    "                              & (np.isfinite(jet1_sqrt_tau21))\n",
    "                              & (np.isfinite(jet2_tau21))\n",
    "                              & (np.isfinite(jet2_tau32))\n",
    "                              & (np.isfinite(jet2_tau43))\n",
    "                              & (np.isfinite(jet2_sqrt_tau21)))[0]\n",
    "\n",
    "    if eta_cut is not None: \n",
    "        bkg_eta_indices = np.where((np.abs(delta_eta) < eta_cut))[0]\n",
    "        bkg_indices = np.intersect1d(bkg_indices, bkg_eta_indices)\n",
    "\n",
    "    if batch_number == 0: \n",
    "        bkg_data = data[bkg_indices]\n",
    "    else: \n",
    "        bkg_data = np.concatenate((bkg_data, data[bkg_indices]), axis=0)\n",
    "    \n",
    "    if box_cox: \n",
    "        \n",
    "        transformed_data = np.zeros(bkg_data.shape)\n",
    "        best_lambdas = []\n",
    "        for col in range(num_features): \n",
    "            boxcox_col, best_lambda = stats.boxcox(bkg_data[:,col] + np.abs(np.min(bkg_data[:,col])) + 1)\n",
    "            transformed_data[:,col] = boxcox_col\n",
    "            best_lambdas.append(best_lambda)\n",
    "\n",
    "        print(best_lambdas)\n",
    "    \n",
    "        bkg_data = transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bkg_data.shape)\n",
    "\n",
    "plot_var = 0\n",
    "plt.hist(bkg_data[:,plot_var], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnorm_bkg_data = np.copy(bkg_data)\n",
    "\n",
    "bkg_mean = []\n",
    "bkg_std = []\n",
    "\n",
    "for index in range(bkg_data.shape[1]):\n",
    "    mean = np.mean(bkg_data[:,index])\n",
    "    std = np.std(bkg_data[:,index])\n",
    "    bkg_mean.append(mean)\n",
    "    bkg_std.append(std)\n",
    "    bkg_data[:,index] = (bkg_data[:,index]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 5000 * num_batches\n",
    "print(\"Batch Size: %s\" % (bs))\n",
    "    \n",
    "total_PureBkg_selection = torch.tensor(bkg_data)\n",
    "bkgAE_train_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs, shuffle=True) \n",
    "bkgAE_test_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device =\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MAF / Planar / NSQUAD / NSRATQUAD\n",
    "class VAE_NF(nn.Module):\n",
    "    def __init__(self, K, D):\n",
    "        super().__init__()\n",
    "        self.dim = D\n",
    "        self.K = K\n",
    "        \n",
    "        '''\n",
    "        self.encoder = nn.Sequential(\n",
    "            #nn.Linear(num_features, 50),\n",
    "            nn.Linear(num_features, 30),\n",
    "            nn.LeakyReLU(True),\n",
    "            #nn.Linear(50, 30),\n",
    "            #nn.LeakyReLU(True),\n",
    "            nn.Linear(30, 20),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(20, D * 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(D, 20),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(20, 30),\n",
    "            nn.LeakyReLU(True),\n",
    "            #nn.Linear(30, 50),\n",
    "            #nn.LeakyReLU(True),\n",
    "            #nn.Linear(50, num_features)\n",
    "            nn.Linear(30, num_features)\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_features, 10),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(10, 8),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(8, D * 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(D, 8),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(8, 10),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(10, num_features)\n",
    "        )\n",
    "        \n",
    "        if flow_type == 'NSQUAD' or flow_type == 'NSRATQUAD': \n",
    "            #----- BEGIN NEW NEURAL SPLINE CODE\n",
    "            \n",
    "            bkg_transforms = []\n",
    "            for _ in range(K):\n",
    "                bkg_transforms.append(ReversePermutation(features=D))\n",
    "                if flow_type == 'NSQUAD': \n",
    "                    bkg_transforms.append(MaskedPiecewiseQuadraticAutoregressiveTransform(features=D, \n",
    "                                                                      hidden_features=NS_hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "                elif flow_type == 'NSRATQUAD': \n",
    "                    bkg_transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(features=D, \n",
    "                                                                      hidden_features=NS_hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "\n",
    "            #bkg_transform = CompositeTransform(bkg_transforms)\n",
    "            bkg_base_dist = MultivariateNormal(torch.zeros(D).cuda(), torch.eye(D).cuda())\n",
    "            self.flows = NormalizingFlowModel(bkg_base_dist, bkg_transforms)\n",
    "            print(self.flows)\n",
    "            \n",
    "            #----- END NEW NEURAL SPLINE CODE\n",
    "        \n",
    "        elif flow_type == 'MAF' or flow_type == 'Planar': \n",
    "            if flow_type == 'MAF': \n",
    "                flow_init = MAF(dim=D)\n",
    "            elif flow_type == 'Planar': \n",
    "                flow_init = Planar(dim=D)\n",
    "            flows_init = [flow_init for _ in range(K)]\n",
    "            prior = MultivariateNormal(torch.zeros(D).cuda(), torch.eye(D).cuda())\n",
    "            self.flows = NormalizingFlowModel(prior, flows_init)\n",
    "            print(self.flows)\n",
    "        \n",
    "        else: \n",
    "            print('ERROR: Flow Type not properly specified.')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run Encoder and get NF params\n",
    "        enc = self.encoder(x)\n",
    "        mu = enc[:, :self.dim]\n",
    "        log_var = enc[:, self.dim: self.dim * 2]\n",
    "\n",
    "        # Re-parametrize\n",
    "        sigma = (log_var * .5).exp()\n",
    "        z = mu + sigma * torch.randn_like(sigma)\n",
    "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        # Construct more expressive posterior with NF\n",
    "        \n",
    "        z_k, _, sum_ladj = self.flows(z)\n",
    "        \n",
    "        kl_div = kl_div / x.size(0) - sum_ladj.mean()  # mean over batch\n",
    "\n",
    "        # Run Decoder\n",
    "        x_prime = self.decoder(z_k)\n",
    "        return x_prime, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global n_steps\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, x in enumerate(bkgAE_train_iterator):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        x = x.float().cuda()\n",
    "\n",
    "        x_tilde, kl_div = model(x)\n",
    "        \n",
    "        mseloss = nn.MSELoss(size_average=False)\n",
    "        \n",
    "        huberloss = nn.SmoothL1Loss(size_average=False)\n",
    "        \n",
    "        #loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "        loss_recons = mseloss(x_tilde,x ) / x.size(0)\n",
    "        \n",
    "        #loss_recons = huberloss(x_tilde,x ) / x.size(0)\n",
    "        loss = loss_recons + beta * kl_div\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append([loss_recons.item(), kl_div.item()])\n",
    "\n",
    "        if (batch_idx + 1) % PRINT_INTERVAL == 0:\n",
    "            print('\\tIter [{}]\\tLoss: {} Time: {:5.3f} ms/batch\\tbeta:{}'.format(\n",
    "                batch_idx * len(x),\n",
    "                np.asarray(train_loss)[-PRINT_INTERVAL:].mean(0),\n",
    "                1000 * (time.time() - start_time),\n",
    "                beta\n",
    "            \n",
    "            ))\n",
    "\n",
    "        n_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(split='valid'):\n",
    "    global n_steps\n",
    "    start_time = time.time()\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, x in enumerate(bkgAE_test_iterator):\n",
    "            \n",
    "            x = x.float().cuda()\n",
    "\n",
    "            x_tilde, kl_div = model(x)\n",
    "            mseloss = nn.MSELoss(size_average=False)\n",
    "            #loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "            huberloss = nn.SmoothL1Loss(size_average=False)\n",
    "        \n",
    "\n",
    "            #loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "            loss_recons = mseloss(x_tilde,x ) / x.size(0)\n",
    "            #loss_recons = huberloss(x_tilde,x ) / x.size(0)\n",
    "            loss = loss_recons + beta * kl_div\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            #writer.add_scalar('loss/{}/ELBO'.format(split), loss.item(), n_steps)\n",
    "            #writer.add_scalar('loss/{}/reconstruction'.format(split), loss_recons.item(), n_steps)\n",
    "            #writer.add_scalar('loss/{}/KL'.format(split), kl_div.item(), n_steps)\n",
    "\n",
    "    print('\\nEvaluation Completed ({})!\\tLoss: {:5.4f}\\tTime: {:5.3f} s'.format(\n",
    "        split,\n",
    "        np.asarray(val_loss).mean(0),\n",
    "        time.time() - start_time\n",
    "    ))\n",
    "    \n",
    "    return np.asarray(val_loss).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdim = [4]\n",
    "nflow = [6]\n",
    "lrs = [1e-3]\n",
    "betas = [1.0]\n",
    "#betas = [0.1,0.5,1.0,2.0,10.0]\n",
    "#betas = [20.0,25.0,50.0,75.0,100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1000\n",
    "PRINT_INTERVAL = 50\n",
    "NUM_WORKERS = 4\n",
    "n_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_losses = []\n",
    "\n",
    "for Z_DIM in zdim:\n",
    "    for N_FLOWS in nflow:\n",
    "        for beta in betas:\n",
    "            model = VAE_NF(N_FLOWS, Z_DIM).cuda()\n",
    "            #NOTE: The \"architecture\" key below can be set to \"MAF\", \"Planar\" (not recommended), \"NSQUAD\", or \"NSRATQUAD\". \n",
    "            ae_def = {\n",
    "                        \"type\":\"qcdbkg\",\n",
    "                        \"trainon\":f\"etacut{re.sub('[.,]', 'p', str(eta_cut))}\",\n",
    "                        \"features\":\"12features\",\n",
    "                        \"architecture\":\"%s\" % (flow_type),\n",
    "                        \"selection\":\"mjjcut\",\n",
    "                        \"trainloss\":\"MSELoss\",\n",
    "                        \"beta\":f\"beta{re.sub('[.,]', 'p', str(beta))}\",\n",
    "                        \"zdimnflow\":f\"z{Z_DIM}f{N_FLOWS}\"\n",
    "                     }\n",
    "            BEST_LOSS = 999999\n",
    "            for LR in lrs:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "                LAST_SAVED = -1\n",
    "                for epoch in range(1, N_EPOCHS + 1):\n",
    "                    print(\"Epoch {}:\".format(epoch))\n",
    "                    train()\n",
    "                    cur_loss = evaluate()\n",
    "                    if beta == 1.0: \n",
    "                        cur_losses.append(cur_loss)\n",
    "\n",
    "                    if cur_loss <= BEST_LOSS:\n",
    "                        PATIENCE_COUNT = 0\n",
    "                        BEST_LOSS = cur_loss\n",
    "                        LAST_SAVED = epoch\n",
    "                        print(\"Saving model!\")\n",
    "                        torch.save(model.state_dict(),f\"/home/myunus/CASE/weights/{ae_def['type']}_{ae_def['trainon']}_{ae_def['features']}_{ae_def['architecture']}_{ae_def['selection']}_{ae_def['trainloss']}_{ae_def['beta']}_{ae_def['zdimnflow']}.h5\")\n",
    "                        #NOTE: Replace the /home/myunus/ above with the directory in which QUASAR resides. \n",
    "\n",
    "                    else:\n",
    "                        PATIENCE_COUNT += 1\n",
    "                        print(\"Not saving model! Last saved: {}\".format(LAST_SAVED))\n",
    "                        if PATIENCE_COUNT > 10:\n",
    "                            print(f\"############Patience Limit Reached with LR={LR}, Best Loss={BEST_LOSS}\")\n",
    "                            break \n",
    "                            \n",
    "                model.load_state_dict(torch.load(f\"/home/myunus/CASE/weights/{ae_def['type']}_{ae_def['trainon']}_{ae_def['features']}_{ae_def['architecture']}_{ae_def['selection']}_{ae_def['trainloss']}_{ae_def['beta']}_{ae_def['zdimnflow']}.h5\"))\n",
    "                #NOTE: Replace the /home/myunus/ above with the directory in which QUASAR resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cur_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Bkg Loss (Beta = 1.0)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_data = np.array([])\n",
    "\n",
    "for batch_number in range(num_batches): \n",
    "    train_batch = \"/nobackup/users/myunus/CASE_samples/BB_batch%s.h5\" % (batch_number)\n",
    "    f = h5py.File(train_batch, \"r\")\n",
    "    \n",
    "    if batch_number == 0: \n",
    "        print(\"Keys: %s\" % f.keys())\n",
    "    \n",
    "    jet_kinematics = f['jet_kinematics']\n",
    "    jet1_extraInfo = f['jet1_extraInfo']\n",
    "    jet2_extraInfo = f['jet2_extraInfo']\n",
    "    truth_label = f['truth_label']\n",
    "\n",
    "    np.seterr(invalid = 'ignore')\n",
    "\n",
    "    delta_eta = jet_kinematics[:,1]\n",
    "\n",
    "    Mjj = np.reshape(jet_kinematics[:,0], (-1,1))\n",
    "    Mj1 = np.reshape(jet_kinematics[:,5], (-1,1))\n",
    "    Mj2 = np.reshape(jet_kinematics[:,9], (-1,1))\n",
    "\n",
    "    jet1_pt = np.reshape(jet_kinematics[:,2], (-1,1))\n",
    "    jet2_pt = np.reshape(jet_kinematics[:,6], (-1,1))\n",
    "\n",
    "    jet1_tau1 = np.reshape(jet1_extraInfo[:,0], (-1,1))\n",
    "    jet1_tau2 = np.reshape(jet1_extraInfo[:,1], (-1,1))\n",
    "    jet1_tau3 = np.reshape(jet1_extraInfo[:,2], (-1,1))\n",
    "    jet1_tau4 = np.reshape(jet1_extraInfo[:,3], (-1,1))\n",
    "    #jet1_btagscore = np.reshape(jet1_extraInfo[:,5],(-1,1))\n",
    "    jet1_numpfconst = np.reshape(jet1_extraInfo[:,6],(-1,1))\n",
    "\n",
    "    jet1_tau21 = jet1_tau2 / jet1_tau1\n",
    "    jet1_tau32 = jet1_tau3 / jet1_tau2\n",
    "    jet1_tau43 = jet1_tau4 / jet1_tau3\n",
    "    jet1_sqrt_tau21 = np.sqrt(jet1_tau21) / jet1_tau1\n",
    "\n",
    "    jet2_tau1 = np.reshape(jet2_extraInfo[:,0], (-1,1))\n",
    "    jet2_tau2 = np.reshape(jet2_extraInfo[:,1], (-1,1))\n",
    "    jet2_tau3 = np.reshape(jet2_extraInfo[:,2], (-1,1))\n",
    "    jet2_tau4 = np.reshape(jet2_extraInfo[:,3], (-1,1))\n",
    "    #jet2_btagscore = np.reshape(jet2_extraInfo[:,5],(-1,1))\n",
    "    jet2_numpfconst = np.reshape(jet2_extraInfo[:,6],(-1,1))\n",
    "\n",
    "    jet2_tau21 = jet2_tau2 / jet2_tau1\n",
    "    jet2_tau32 = jet2_tau3 / jet2_tau2\n",
    "    jet2_tau43 = jet2_tau4 / jet2_tau3\n",
    "    jet2_sqrt_tau21 = np.sqrt(jet2_tau21) / jet2_tau1\n",
    "\n",
    "    truth_label = truth_label[:]\n",
    "    \n",
    "    data = np.concatenate((jet1_tau21, jet1_tau32, jet1_tau43, jet2_tau21, jet2_tau32, jet2_tau43), axis=1)\n",
    "\n",
    "    sig_indices = np.where((truth_label == 2) \n",
    "                              & (Mjj > Mjj_cut) \n",
    "                              & (jet1_pt > pt_cut) \n",
    "                              & (jet2_pt > pt_cut)\n",
    "                              & (np.isfinite(jet1_tau21))\n",
    "                              & (np.isfinite(jet1_tau32))\n",
    "                              & (np.isfinite(jet1_tau43))\n",
    "                              & (np.isfinite(jet1_sqrt_tau21))\n",
    "                              & (np.isfinite(jet2_tau21))\n",
    "                              & (np.isfinite(jet2_tau32))\n",
    "                              & (np.isfinite(jet2_tau43))\n",
    "                              & (np.isfinite(jet2_sqrt_tau21)))[0]\n",
    "\n",
    "    if eta_cut is not None: \n",
    "        sig_eta_indices = np.where((np.abs(delta_eta) < eta_cut))[0]\n",
    "        sig_indices = np.intersect1d(sig_indices, sig_eta_indices)\n",
    "\n",
    "    if batch_number == 0: \n",
    "        sig_data = data[sig_indices]\n",
    "    else: \n",
    "        sig_data = np.concatenate((sig_data, data[sig_indices]), axis=0)\n",
    "        \n",
    "    if box_cox: \n",
    "        \n",
    "        transformed_data = np.zeros(sig_data.shape)\n",
    "        best_lambdas = []\n",
    "        for col in range(num_features): \n",
    "            boxcox_col, best_lambda = stats.boxcox(sig_data[:,col] + np.abs(np.min(sig_data[:,col])) + 1)\n",
    "            transformed_data[:,col] = boxcox_col\n",
    "            best_lambdas.append(best_lambda)\n",
    "\n",
    "        print(best_lambdas)\n",
    "\n",
    "        sig_data = transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sig_data.shape)\n",
    "\n",
    "plot_var = 0\n",
    "plt.hist(sig_data[:,plot_var], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnorm_sig_data = np.copy(sig_data)\n",
    "\n",
    "sig_mean = []\n",
    "sig_std = []\n",
    "\n",
    "for index in range(sig_data.shape[1]):\n",
    "    mean = np.mean(sig_data[:,index])\n",
    "    std = np.std(sig_data[:,index])\n",
    "    sig_mean.append(mean)\n",
    "    sig_std.append(std)\n",
    "    sig_data[:,index] = (sig_data[:,index]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_result(object):\n",
    "    \n",
    "    def __init__(self, prefix, aetype):\n",
    "        self.sigloss = np.load(prefix+'_'+sigloss+'.npy')\n",
    "        self.bkgloss = np.load(prefix+'_'+bkgloss+'.npy')\n",
    "        self.aetype = aetype\n",
    "\n",
    "        \n",
    "    def get_tpr_fpr(self):\n",
    "        bins = np.linspace(0,10000,100001)\n",
    "        tpr = []\n",
    "        fpr = []\n",
    "        for cut in bins:\n",
    "            if self.aetype == 'sig':\n",
    "                tpr.append(np.where(self.sigloss<cut)[0].shape[0]/len(self.sigloss))\n",
    "                fpr.append(np.where(self.bkgloss<cut)[0].shape[0]/len(self.bkgloss))\n",
    "            if self.aetype == 'bkg':\n",
    "                tpr.append(np.where(self.sigloss>cut)[0].shape[0]/len(self.sigloss))\n",
    "                fpr.append(np.where(self.bkgloss>cut)[0].shape[0]/len(self.bkgloss))\n",
    "        \n",
    "\n",
    "        return tpr,fpr\n",
    "    \n",
    "    def get_precision_recall(self):\n",
    "        bins = np.linspace(0,1000,10001)\n",
    "        tpr = []\n",
    "        fpr = []\n",
    "        precision = []\n",
    "        for cut in bins:\n",
    "            if self.aetype == 'sig':\n",
    "                tpr.append(np.where(self.sigloss<cut)[0].shape[0]/len(self.sigloss))\n",
    "                precision.append((np.where(self.sigloss<cut)[0].shape[0])/(np.where(self.bkgloss<cut)[0].shape[0]+np.where(self.sigloss<cut)[0].shape[0]))\n",
    "            \n",
    "            if self.aetype == 'bkg':\n",
    "                tpr.append(np.where(self.sigloss>cut)[0].shape[0]/len(self.sigloss))\n",
    "                precision.append((np.where(self.sigloss>cut)[0].shape[0])/(np.where(self.bkgloss>cut)[0].shape[0]+np.where(self.sigloss>cut)[0].shape[0]))\n",
    "        \n",
    "\n",
    "        return precision,tpr  \n",
    "\n",
    "    def FPRat95TPR(self):\n",
    "        tprs, fprs = get_tpr_fpr(self)\n",
    "        for i in range(len(tprs)-1):\n",
    "            if (tprs[i] < 0.95) and (tprs[i+1] >= 0.95):\n",
    "                return fprs[i+1]\n",
    "\n",
    "    def FPRat99TPR(self):\n",
    "        tprs, fprs = get_tpr_fpr(self)\n",
    "        for i in range(len(tprs) - 1):\n",
    "            if (tprs[i] < 0.99) and (tprs[i + 1] >= 0.99):\n",
    "                return fprs[i+1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_test = torch.tensor(bkg_data)\n",
    "sig_test = torch.tensor(sig_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_fpr(sigloss,bkgloss):\n",
    "    bins = np.linspace(0,100,10001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for cut in bins:\n",
    "        tpr.append(np.where(sigloss>cut)[0].shape[0]/len(sigloss))\n",
    "        fpr.append(np.where(bkgloss>cut)[0].shape[0]/len(bkgloss))\n",
    "\n",
    "\n",
    "    return tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprlist = []\n",
    "fprlist = []\n",
    "namelist = []\n",
    "bkgtr_siglosslist = []\n",
    "bkgtr_bkglosslist = []\n",
    "tprlist_forinverse = []\n",
    "fprinverselist = []\n",
    "\n",
    "for Z_DIM in zdim:\n",
    "    for N_FLOWS in nflow:\n",
    "        for beta in betas:\n",
    "            model = VAE_NF(N_FLOWS, Z_DIM).cuda()\n",
    "            #NOTE: The \"architecture\" key below can be set to \"MAF\", \"Planar\" (not recommended), \"NSQUAD\", or \"NSRATQUAD\". \n",
    "            ae_def = {\n",
    "                        \"type\":\"qcdbkg\",\n",
    "                        \"trainon\":f\"etacut{re.sub('[.,]', 'p', str(eta_cut))}\",\n",
    "                        \"features\":\"12features\",\n",
    "                        \"architecture\":\"%s\" % (flow_type),\n",
    "                        \"selection\":\"mjjcut\",\n",
    "                        \"trainloss\":\"MSELoss\",\n",
    "                        \"beta\":f\"beta{re.sub('[.,]', 'p', str(beta))}\",\n",
    "                        \"zdimnflow\":f\"z{Z_DIM}f{N_FLOWS}\"\n",
    "                     }\n",
    "            model.load_state_dict(torch.load(f\"/home/myunus/CASE/weights/{ae_def['type']}_{ae_def['trainon']}_{ae_def['features']}_{ae_def['architecture']}_{ae_def['selection']}_{ae_def['trainloss']}_{ae_def['beta']}_{ae_def['zdimnflow']}.h5\"), strict=False)\n",
    "            #NOTE: Replace the /home/myunus/ above with the directory in which QUASAR resides.\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                sig_loss = torch.mean((model(sig_test.float().cuda())[0]- sig_test.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "                bkg_loss = torch.mean((model(bkg_test.float().cuda())[0]- bkg_test.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "                #sig_loss = torch.mean((model(sig_test.float().cuda())[0]- sig_test.float().cuda())**2,dim=1).data.cpu().numpy() + beta * F.kl_div(model(sig_test.float().cuda())[0], sig_test.float().cuda()).cpu().numpy()\n",
    "                #bkg_loss = torch.mean((model(bkg_test.float().cuda())[0]- bkg_test.float().cuda())**2,dim=1).data.cpu().numpy() + beta * F.kl_div(model(bkg_test.float().cuda())[0], bkg_test.float().cuda()).cpu().numpy()\n",
    "                    \n",
    "            if beta == 1.0: \n",
    "                #hi_loss_sig_indices = np.argwhere(sig_loss > 1).flatten()\n",
    "                #lo_loss_sig_indices = np.argwhere(sig_loss < 1).flatten()\n",
    "                \n",
    "                #hi_loss_sig_mj1 = unnorm_sig_data[hi_loss_sig_indices][:,0]\n",
    "                #lo_loss_sig_mj1 = unnorm_sig_data[lo_loss_sig_indices][:,0]\n",
    "                \n",
    "                #hi_loss_sig_mj2 = unnorm_sig_data[hi_loss_sig_indices][:,7]\n",
    "                #lo_loss_sig_mj2 = unnorm_sig_data[lo_loss_sig_indices][:,7]\n",
    "                \n",
    "                plt.hist(sig_loss, bins=50)\n",
    "                plt.xlabel('sig loss (beta = 1.0)')\n",
    "                plt.show()\n",
    "\n",
    "                plt.hist(bkg_loss, bins=50)\n",
    "                plt.xlabel('bkg loss (beta = 1.0)')\n",
    "                plt.show()\n",
    "                \n",
    "                '''\n",
    "                plt.hist(hi_loss_sig_mj1, bins=50)\n",
    "                plt.xlabel('sig mj1 (loss > 1)')\n",
    "                plt.show()\n",
    "                \n",
    "                plt.hist(lo_loss_sig_mj1, bins=50)\n",
    "                plt.xlabel('sig mj1 (loss < 1)')\n",
    "                plt.show()\n",
    "                \n",
    "                plt.hist(hi_loss_sig_mj2, bins=50)\n",
    "                plt.xlabel('sig mj2 (loss > 1)')\n",
    "                plt.show()\n",
    "                \n",
    "                plt.hist(lo_loss_sig_mj2, bins=50)\n",
    "                plt.xlabel('sig mj2 (loss < 1)')\n",
    "                plt.show()\n",
    "                '''\n",
    "            \n",
    "            np.save(f\"/home/myunus/CASE/data_strings/{ae_def['type']}_{ae_def['trainon']}_{ae_def['features']}_{ae_def['architecture']}_{ae_def['selection']}_{ae_def['trainloss']}_{ae_def['beta']}_{ae_def['zdimnflow']}_sigloss.npy\", sig_loss)\n",
    "            #NOTE: Replace the /home/myunus/ above with the directory in which QUASAR resides.\n",
    "            np.save(f\"/home/myunus/CASE/data_strings/{ae_def['type']}_{ae_def['trainon']}_{ae_def['features']}_{ae_def['architecture']}_{ae_def['selection']}_{ae_def['trainloss']}_{ae_def['beta']}_{ae_def['zdimnflow']}_bkgloss.npy\", bkg_loss)\n",
    "            #NOTE: Replace the /home/myunus/ above with the directory in which QUASAR resides.\n",
    "            \n",
    "            namelist.append(ae_def)\n",
    "            tpr, fpr = get_tpr_fpr(sig_loss,bkg_loss)\n",
    "            tprlist.append(tpr)\n",
    "            fprlist.append(fpr)\n",
    "            tpr_np, fpr_np = np.array(tpr), np.array(fpr)\n",
    "            \n",
    "            nonzero_idx = np.nonzero(fpr_np)\n",
    "            \n",
    "            bkgtr_siglosslist.append(sig_loss)\n",
    "            bkgtr_bkglosslist.append(bkg_loss)\n",
    "            \n",
    "            tprlist_forinverse.append(tpr_np[nonzero_idx])\n",
    "            fprinverselist.append(1/fpr_np[nonzero_idx]) \n",
    "            \n",
    "            if beta == 1.0: \n",
    "                for plot_var in range(num_features):\n",
    "                    print(bkg_test.cpu().numpy().shape)\n",
    "                    print(model(bkg_test.float().cuda())[0].data.cpu().numpy().shape)\n",
    "                    n, bins, patches = plt.hist((bkg_test.cpu().numpy() * bkg_std + bkg_mean)[:,plot_var], bins = 50, alpha = 0.75)\n",
    "                    plt.hist((model(bkg_test.float().cuda())[0].data.cpu().numpy() * bkg_std + bkg_mean)[:,plot_var], bins = bins, alpha = 0.75)\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(betas)): \n",
    "        \n",
    "    bkgtr_bkgloss = bkgtr_bkglosslist[index]\n",
    "    bkgtr_sigloss = bkgtr_siglosslist[index]\n",
    "    \n",
    "    df_bkgloss = pd.DataFrame(bkgtr_bkgloss)\n",
    "    df_sigloss = pd.DataFrame(bkgtr_sigloss)\n",
    "    df_bkgloss.to_csv('csv_files/bkgtr_bkgloss_%s_%s_%s.csv' % (namelist[index]['trainon'], namelist[index]['architecture'], namelist[index]['beta'])) \n",
    "    df_sigloss.to_csv('csv_files/bkgtr_sigloss_%s_%s_%s.csv' % (namelist[index]['trainon'], namelist[index]['architecture'], namelist[index]['beta']))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "for tpr, fpr, name,sigloss,bkgloss in zip(tprlist_forinverse,fprinverselist, namelist,bkgtr_siglosslist,bkgtr_bkglosslist):\n",
    "    #if name['beta'] == 'beta10p0' or name['beta'] == 'beta2p0':\n",
    "    if name['zdimnflow'] == 'z4f6':\n",
    "        #print(tpr, fpr)\n",
    "        plt.plot(tpr,fpr, label=f\"{name['beta']}_{name['zdimnflow']}\")\n",
    "        print(f\"{name['beta']}_{name['zdimnflow']}\",metrics.auc(fpr,tpr))\n",
    "        #plt.hist(sigloss,np.arange(0,10,0.1),alpha=0.2, density=True, label=f\"{name['beta']}_{name['zdimnflow']}\")\n",
    "        #plt.hist(bkgloss,np.arange(0,10,0.1),alpha=0.2, density=True, label=f\"{name['beta']}_{name['zdimnflow']}\")\n",
    "plt.xlabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "plt.ylabel(r'$1/\\epsilon_{bkg}$',fontsize=15)\n",
    "#plt.semilogy()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.title('Background Prior')\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim([0.0,1.0])\n",
    "#plt.ylim([0.0,1.0])\n",
    "#plt.savefig('ROC_effectiveness_of_quak.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tpr, fpr, name,sigloss,bkgloss in zip(tprlist,fprlist, namelist,bkgtr_siglosslist,bkgtr_bkglosslist):\n",
    "    #if name['beta'] == 'beta10p0' or name['beta'] == 'beta2p0':\n",
    "    if name['zdimnflow'] == 'z4f6':\n",
    "        plt.plot(fpr,tpr, label=f\"{name['beta']}_{name['zdimnflow']}\")\n",
    "        print(f\"{name['beta']}_{name['zdimnflow']}\",metrics.auc(fpr,tpr))\n",
    "        #plt.hist(sigloss,np.arange(0,10,0.1),alpha=0.2, density=True, label=f\"{name['beta']}_{name['zdimnflow']}\")\n",
    "        #plt.hist(bkgloss,np.arange(0,10,0.1),alpha=0.2, density=True, label=f\"{name['beta']}_{name['zdimnflow']}\")\n",
    "plt.xlabel(r'$\\epsilon_{bkg}$',fontsize=15)\n",
    "plt.ylabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "#plt.semilogy()\n",
    "#plt.yscale('log')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "#plt.legend(loc='lower right')\n",
    "#plt.xlim([0.05,1.0])\n",
    "#plt.ylim([0.0,1.0])\n",
    "#plt.savefig('ROC_effectiveness_of_quak.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
