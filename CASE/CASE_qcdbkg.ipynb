{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_available_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Number of available CPUs:\", num_available_cpus)\n",
    "\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../new_flows\")\n",
    "from flows import RealNVP, Planar, MAF\n",
    "from models import NormalizingFlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.flows.autoregressive import MaskedAutoregressiveFlow\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform, MaskedPiecewiseQuadraticAutoregressiveTransform, MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1\n",
    "\n",
    "Mjj_cut = 1200\n",
    "pt_cut = 550\n",
    "eta_cut = None\n",
    "box_cox = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 12\n",
    "NS_hidden_features = 48\n",
    "flow_type = 'NSQUAD' #Options are 'MAF', 'Planar' (not recommended), 'NSQUAD', and 'NSRATQUAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device =\", device)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor') if torch.cuda.is_available() else print ('cpu')\n",
    "\n",
    "torch.set_num_threads(num_available_cpus)\n",
    "\n",
    "print(\"Number of threads:\", torch.get_num_threads())\n",
    "print(\"Number of interop threads:\", torch.get_num_interop_threads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_data = np.array([])\n",
    "\n",
    "for batch_number in range(num_batches): \n",
    "    train_batch = \"/nobackup/users/myunus/CASE_samples/BB_batch%s.h5\" % (batch_number)\n",
    "    f = h5py.File(train_batch, \"r\")\n",
    "    \n",
    "    if batch_number == 0: \n",
    "        print(\"Keys: %s\" % f.keys())\n",
    "    \n",
    "    jet_kinematics = f['jet_kinematics']\n",
    "    jet1_extraInfo = f['jet1_extraInfo']\n",
    "    jet2_extraInfo = f['jet2_extraInfo']\n",
    "    truth_label = f['truth_label']\n",
    "\n",
    "    np.seterr(invalid = 'ignore')\n",
    "\n",
    "    delta_eta = jet_kinematics[:,1]\n",
    "\n",
    "    Mjj = np.reshape(jet_kinematics[:,0], (-1,1))\n",
    "    Mj1 = np.reshape(jet_kinematics[:,5], (-1,1))\n",
    "    Mj2 = np.reshape(jet_kinematics[:,9], (-1,1))\n",
    "\n",
    "    jet1_pt = np.reshape(jet_kinematics[:,2], (-1,1))\n",
    "    jet2_pt = np.reshape(jet_kinematics[:,6], (-1,1))\n",
    "\n",
    "    jet1_tau1 = np.reshape(jet1_extraInfo[:,0], (-1,1))\n",
    "    jet1_tau2 = np.reshape(jet1_extraInfo[:,1], (-1,1))\n",
    "    jet1_tau3 = np.reshape(jet1_extraInfo[:,2], (-1,1))\n",
    "    jet1_tau4 = np.reshape(jet1_extraInfo[:,3], (-1,1))\n",
    "    #jet1_btagscore = np.reshape(jet1_extraInfo[:,5],(-1,1))\n",
    "    jet1_numpfconst = np.reshape(jet1_extraInfo[:,6],(-1,1))\n",
    "\n",
    "    jet1_tau21 = jet1_tau2 / jet1_tau1\n",
    "    jet1_tau32 = jet1_tau3 / jet1_tau2\n",
    "    jet1_tau43 = jet1_tau4 / jet1_tau3\n",
    "    jet1_sqrt_tau21 = np.sqrt(jet1_tau21) / jet1_tau1\n",
    "\n",
    "    jet2_tau1 = np.reshape(jet2_extraInfo[:,0], (-1,1))\n",
    "    jet2_tau2 = np.reshape(jet2_extraInfo[:,1], (-1,1))\n",
    "    jet2_tau3 = np.reshape(jet2_extraInfo[:,2], (-1,1))\n",
    "    jet2_tau4 = np.reshape(jet2_extraInfo[:,3], (-1,1))\n",
    "    #jet2_btagscore = np.reshape(jet2_extraInfo[:,5],(-1,1))\n",
    "    jet2_numpfconst = np.reshape(jet2_extraInfo[:,6],(-1,1))\n",
    "\n",
    "    jet2_tau21 = jet2_tau2 / jet2_tau1\n",
    "    jet2_tau32 = jet2_tau3 / jet2_tau2\n",
    "    jet2_tau43 = jet2_tau4 / jet2_tau3\n",
    "    jet2_sqrt_tau21 = np.sqrt(jet2_tau21) / jet2_tau1\n",
    "\n",
    "    truth_label = truth_label[:]\n",
    "    \n",
    "    data = np.concatenate((Mj1, jet1_tau21, jet1_tau32, jet1_tau43, jet1_sqrt_tau21, jet1_numpfconst, \n",
    "                       Mj2, jet2_tau21, jet2_tau32, jet2_tau43, jet2_sqrt_tau21, jet2_numpfconst), axis=1)\n",
    "\n",
    "    bkg_indices = np.where((truth_label == 0) \n",
    "                              & (Mjj > Mjj_cut) \n",
    "                              & (Mj1 > 0)\n",
    "                              & (Mj2 > 0)\n",
    "                              & (jet1_pt > pt_cut) \n",
    "                              & (jet2_pt > pt_cut)\n",
    "                              & (np.isfinite(jet1_tau21))\n",
    "                              & (np.isfinite(jet1_tau32))\n",
    "                              & (np.isfinite(jet1_tau43))\n",
    "                              & (np.isfinite(jet1_sqrt_tau21))\n",
    "                              & (np.isfinite(jet2_tau21))\n",
    "                              & (np.isfinite(jet2_tau32))\n",
    "                              & (np.isfinite(jet2_tau43))\n",
    "                              & (np.isfinite(jet2_sqrt_tau21)))[0]\n",
    "\n",
    "    if eta_cut is not None: \n",
    "        bkg_eta_indices = np.where((np.abs(delta_eta) < eta_cut))[0]\n",
    "        bkg_indices = np.intersect1d(bkg_indices, bkg_eta_indices)\n",
    "\n",
    "    if batch_number == 0: \n",
    "        bkg_data = data[bkg_indices]\n",
    "    else: \n",
    "        bkg_data = np.concatenate((bkg_data, data[bkg_indices]), axis=0)\n",
    "    \n",
    "    if box_cox: \n",
    "        \n",
    "        transformed_data = np.zeros(bkg_data.shape)\n",
    "        best_lambdas = []\n",
    "        for col in range(num_features): \n",
    "            boxcox_col, best_lambda = stats.boxcox(bkg_data[:,col] + np.abs(np.min(bkg_data[:,col])) + 1)\n",
    "            transformed_data[:,col] = boxcox_col\n",
    "            best_lambdas.append(best_lambda)\n",
    "\n",
    "        print(best_lambdas)\n",
    "    \n",
    "        bkg_data = transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bkg_data.shape)\n",
    "\n",
    "plot_var = 0\n",
    "plt.hist(bkg_data[:,plot_var], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_mean = []\n",
    "bkg_std = []\n",
    "\n",
    "for index in range(bkg_data.shape[1]):\n",
    "    mean = np.mean(bkg_data[:,index])\n",
    "    std = np.std(bkg_data[:,index])\n",
    "    bkg_mean.append(mean)\n",
    "    bkg_std.append(std)\n",
    "    bkg_data[:,index] = (bkg_data[:,index]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 5000 * num_batches\n",
    "print(\"Batch Size: %s\" % (bs))\n",
    "    \n",
    "total_PureBkg_selection = torch.tensor(bkg_data)\n",
    "bkgAE_train_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs, shuffle=True) \n",
    "bkgAE_test_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device =\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MAF / Planar / NSQUAD / NSRATQUAD\n",
    "class VAE_NF(nn.Module):\n",
    "    def __init__(self, K, D):\n",
    "        super().__init__()\n",
    "        self.dim = D\n",
    "        self.K = K\n",
    "        \n",
    "        '''\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_features, 50),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(50, 30),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(30, 20),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(20, D * 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(D, 20),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(20, 30),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(30, 50),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(50, num_features)\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_features, 30),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(30, 20),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(20, D * 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(D, 20),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(20, 30),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(30, num_features)\n",
    "        )\n",
    "        \n",
    "        if flow_type == 'NSQUAD' or flow_type == 'NSRATQUAD': \n",
    "            #----- BEGIN NEW NEURAL SPLINE CODE\n",
    "            \n",
    "            bkg_transforms = []\n",
    "            for _ in range(K):\n",
    "                bkg_transforms.append(ReversePermutation(features=D))\n",
    "                if flow_type == 'NSQUAD': \n",
    "                    bkg_transforms.append(MaskedPiecewiseQuadraticAutoregressiveTransform(features=D, \n",
    "                                                                      hidden_features=NS_hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "                elif flow_type == 'NSRATQUAD': \n",
    "                    bkg_transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(features=D, \n",
    "                                                                      hidden_features=NS_hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "\n",
    "            #bkg_transform = CompositeTransform(bkg_transforms)\n",
    "            bkg_base_dist = MultivariateNormal(torch.zeros(D).cuda(), torch.eye(D).cuda())\n",
    "            self.flows = NormalizingFlowModel(bkg_base_dist, bkg_transforms)\n",
    "            print(self.flows)\n",
    "            \n",
    "            #----- END NEW NEURAL SPLINE CODE\n",
    "        \n",
    "        elif flow_type == 'MAF' or flow_type == 'Planar': \n",
    "            if flow_type == 'MAF': \n",
    "                flow_init = MAF(dim=D)\n",
    "            elif flow_type == 'Planar': \n",
    "                flow_init = Planar(dim=D)\n",
    "            flows_init = [flow_init for _ in range(K)]\n",
    "            prior = MultivariateNormal(torch.zeros(D).cuda(), torch.eye(D).cuda())\n",
    "            self.flows = NormalizingFlowModel(prior, flows_init)\n",
    "            print(self.flows)\n",
    "        \n",
    "        else: \n",
    "            print('ERROR: Flow Type not properly specified.')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run Encoder and get NF params\n",
    "        enc = self.encoder(x)\n",
    "        mu = enc[:, :self.dim]\n",
    "        log_var = enc[:, self.dim: self.dim * 2]\n",
    "\n",
    "        # Re-parametrize\n",
    "        sigma = (log_var * .5).exp()\n",
    "        z = mu + sigma * torch.randn_like(sigma)\n",
    "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        # Construct more expressive posterior with NF\n",
    "        \n",
    "        z_k, _, sum_ladj = self.flows(z)\n",
    "        \n",
    "        kl_div = kl_div / x.size(0) - sum_ladj.mean()  # mean over batch\n",
    "\n",
    "        # Run Decoder\n",
    "        x_prime = self.decoder(z_k)\n",
    "        return x_prime, kl_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global n_steps\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, x in enumerate(bkgAE_train_iterator):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        x = x.float().cuda()\n",
    "\n",
    "        x_tilde, kl_div = model(x)\n",
    "        \n",
    "        mseloss = nn.MSELoss(size_average=False)\n",
    "        \n",
    "        huberloss = nn.SmoothL1Loss(size_average=False)\n",
    "        \n",
    "        #loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "        loss_recons = mseloss(x_tilde,x ) / x.size(0)\n",
    "        \n",
    "        #loss_recons = huberloss(x_tilde,x ) / x.size(0)\n",
    "        loss = loss_recons + beta * kl_div\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append([loss_recons.item(), kl_div.item()])\n",
    "\n",
    "        if (batch_idx + 1) % PRINT_INTERVAL == 0:\n",
    "            print('\\tIter [{}]\\tLoss: {} Time: {:5.3f} ms/batch\\tbeta:{}'.format(\n",
    "                batch_idx * len(x),\n",
    "                np.asarray(train_loss)[-PRINT_INTERVAL:].mean(0),\n",
    "                1000 * (time.time() - start_time),\n",
    "                beta\n",
    "            \n",
    "            ))\n",
    "\n",
    "        n_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(split='valid'):\n",
    "    global n_steps\n",
    "    start_time = time.time()\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, x in enumerate(bkgAE_test_iterator):\n",
    "            \n",
    "            x = x.float().cuda()\n",
    "\n",
    "            x_tilde, kl_div = model(x)\n",
    "            mseloss = nn.MSELoss(size_average=False)\n",
    "            #loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "            huberloss = nn.SmoothL1Loss(size_average=False)\n",
    "        \n",
    "\n",
    "            #loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "            loss_recons = mseloss(x_tilde,x ) / x.size(0)\n",
    "            #loss_recons = huberloss(x_tilde,x ) / x.size(0)\n",
    "            loss = loss_recons + beta * kl_div\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            #writer.add_scalar('loss/{}/ELBO'.format(split), loss.item(), n_steps)\n",
    "            #writer.add_scalar('loss/{}/reconstruction'.format(split), loss_recons.item(), n_steps)\n",
    "            #writer.add_scalar('loss/{}/KL'.format(split), kl_div.item(), n_steps)\n",
    "\n",
    "    print('\\nEvaluation Completed ({})!\\tLoss: {:5.4f}\\tTime: {:5.3f} s'.format(\n",
    "        split,\n",
    "        np.asarray(val_loss).mean(0),\n",
    "        time.time() - start_time\n",
    "    ))\n",
    "    \n",
    "    return np.asarray(val_loss).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdim = [4]\n",
    "nflow = [10]\n",
    "lrs = [1e-3]\n",
    "betas = [0.0,0.01,0.05]\n",
    "#betas = [0.1,0.5,1.0,2.0,10.0]\n",
    "#betas = [20.0,25.0,50.0,75.0,100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1000\n",
    "PRINT_INTERVAL = 50\n",
    "NUM_WORKERS = 4\n",
    "n_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_losses = []\n",
    "\n",
    "for Z_DIM in zdim:\n",
    "    for N_FLOWS in nflow:\n",
    "        for beta in betas:\n",
    "            model = VAE_NF(N_FLOWS, Z_DIM).cuda()\n",
    "            #NOTE: The \"architecture\" key below can be set to \"MAF\", \"Planar\" (not recommended), \"NSQUAD\", or \"NSRATQUAD\". \n",
    "            ae_def = {\n",
    "                        \"type\":\"qcdbkg\",\n",
    "                        \"trainon\":f\"etacut{re.sub('[.,]', 'p', str(eta_cut))}\",\n",
    "                        \"features\":\"12features\",\n",
    "                        \"architecture\":\"%s\" % (flow_type),\n",
    "                        \"selection\":\"mjjcut\",\n",
    "                        \"trainloss\":\"MSELoss\",\n",
    "                        \"beta\":f\"beta{re.sub('[.,]', 'p', str(beta))}\",\n",
    "                        \"zdimnflow\":f\"z{Z_DIM}f{N_FLOWS}\"\n",
    "                     }\n",
    "            BEST_LOSS = 999999\n",
    "            for LR in lrs:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "                LAST_SAVED = -1\n",
    "                for epoch in range(1, N_EPOCHS + 1):\n",
    "                    print(\"Epoch {}:\".format(epoch))\n",
    "                    train()\n",
    "                    cur_loss = evaluate()\n",
    "                    if beta == 1.0: \n",
    "                        cur_losses.append(cur_loss)\n",
    "\n",
    "                    if cur_loss <= BEST_LOSS:\n",
    "                        PATIENCE_COUNT = 0\n",
    "                        BEST_LOSS = cur_loss\n",
    "                        LAST_SAVED = epoch\n",
    "                        print(\"Saving model!\")\n",
    "                        torch.save(model.state_dict(),f\"/home/myunus/CASE/weights/{ae_def['type']}_{ae_def['trainon']}_{ae_def['features']}_{ae_def['architecture']}_{ae_def['selection']}_{ae_def['trainloss']}_{ae_def['beta']}_{ae_def['zdimnflow']}.h5\")\n",
    "                        #NOTE: Replace the /home/myunus/ above with the directory in which QUASAR resides. \n",
    "\n",
    "                    else:\n",
    "                        PATIENCE_COUNT += 1\n",
    "                        print(\"Not saving model! Last saved: {}\".format(LAST_SAVED))\n",
    "                        if PATIENCE_COUNT > 10:\n",
    "                            print(f\"############Patience Limit Reached with LR={LR}, Best Loss={BEST_LOSS}\")\n",
    "                            break \n",
    "                            \n",
    "                model.load_state_dict(torch.load(f\"/home/myunus/CASE/weights/{ae_def['type']}_{ae_def['trainon']}_{ae_def['features']}_{ae_def['architecture']}_{ae_def['selection']}_{ae_def['trainloss']}_{ae_def['beta']}_{ae_def['zdimnflow']}.h5\"))\n",
    "                #NOTE: Replace the /home/myunus/ above with the directory in which QUASAR resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cur_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Bkg Loss (Beta = 1.0)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
