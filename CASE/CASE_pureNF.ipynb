{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_available_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Number of available CPUs:\", num_available_cpus)\n",
    "\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../new_flows\")\n",
    "from flows import RealNVP, Planar, MAF\n",
    "from models import NormalizingFlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.flows.autoregressive import MaskedAutoregressiveFlow\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform, MaskedPiecewiseQuadraticAutoregressiveTransform, MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process bkg samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1\n",
    "\n",
    "Mjj_cut = 1200\n",
    "pt_cut = 550\n",
    "eta_cut = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device =\", device)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor') if torch.cuda.is_available() else print ('cpu')\n",
    "\n",
    "torch.set_num_threads(num_available_cpus)\n",
    "\n",
    "print(\"Number of threads:\", torch.get_num_threads())\n",
    "print(\"Number of interop threads:\", torch.get_num_interop_threads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_data, bkg_unnorm_data, bkg_masses = LAPS_train(sample_type = 'qcdbkg', num_batches = num_batches)\n",
    "\n",
    "bkg_mean = np.mean(bkg_unnorm_data, axis=0)\n",
    "bkg_std = np.std(bkg_unnorm_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bkg_masses.shape)\n",
    "\n",
    "plt.hist(bkg_masses, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureBkg = torch.tensor(bkg_data)\n",
    "total_PureBkg_selection = total_PureBkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureBkg_selection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10000 * num_batches\n",
    "bkgAE_train_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs, shuffle=True) \n",
    "bkgAE_test_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the bkg-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 14\n",
    "hidden_features = 56\n",
    "\n",
    "num_layers = 4\n",
    "num_blocks_per_layer = 4\n",
    "#num_iter = 10000\n",
    "num_iter = 1000\n",
    "print_interval = 20\n",
    "\n",
    "#Current flow_type options: 'MAF', 'NSQUAD' (neural spline quadratic), 'NSRATQUAD' (neural spline rational quadratic)\n",
    "flow_type = 'NSQUAD'\n",
    "\n",
    "save_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device =\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(bkg_data[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_loss_dict = dict()\n",
    "bkg_flow_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Pure_NF_%s_k%s_hf%s_nbpl%s' % (flow_type, num_layers, hidden_features, num_blocks_per_layer)\n",
    "\n",
    "print(\"FCNN Hidden Layer Width: \", hidden_features)\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "bkg_base_dist = StandardNormal(shape=[num_features])\n",
    "\n",
    "bkg_transforms = []\n",
    "for _ in range(num_layers):\n",
    "    bkg_transforms.append(ReversePermutation(features=num_features))\n",
    "    if flow_type == 'MAF': \n",
    "        bkg_transforms.append(MaskedAffineAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features))\n",
    "    elif flow_type == 'NSQUAD': \n",
    "        bkg_transforms.append(MaskedPiecewiseQuadraticAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "    elif flow_type == 'NSRATQUAD': \n",
    "        bkg_transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "\n",
    "bkg_transform = CompositeTransform(bkg_transforms)\n",
    "\n",
    "bkg_flow = Flow(bkg_transform, bkg_base_dist)\n",
    "#print(bkg_flow)\n",
    "\n",
    "bkg_optimizer = optim.Adam(bkg_flow.parameters())\n",
    "\n",
    "bkg_tick = time.time()\n",
    "\n",
    "bkg_min_loss = 999999\n",
    "bkg_best_flow = None\n",
    "\n",
    "cur_losses = []\n",
    "patience_count = 0\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    terminate = False\n",
    "\n",
    "    for batch_idx, x in enumerate(bkgAE_train_iterator):\n",
    "\n",
    "        #x, y = datasets.make_moons(1024, noise=.1)\n",
    "        #x = bkg_tr_data\n",
    "        #x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        bkg_optimizer.zero_grad()\n",
    "        loss = -bkg_flow.log_prob(inputs=x)[0].mean()\n",
    "        loss.backward()\n",
    "        bkg_optimizer.step()\n",
    "\n",
    "        if batch_idx == len(bkgAE_train_iterator) - 1 :\n",
    "\n",
    "            xline = torch.linspace(-2, 2)\n",
    "            yline = torch.linspace(-2, 2)\n",
    "            xgrid, ygrid = torch.meshgrid(xline, yline)\n",
    "            xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #bkg_zgrid = bkg_flow.log_prob(xyinput)[0].exp().reshape(100, 100)\n",
    "                bkg_zgrid = -bkg_flow.log_prob(x)[0].cpu().numpy()\n",
    "\n",
    "                #print(bkg_zgrid.shape)\n",
    "                #bkg_zgrid = bkg_zgrid[bkg_zgrid < 10]\n",
    "                #print(bkg_zgrid.shape)\n",
    "                #print(bkg_zgrid[:5])\n",
    "\n",
    "            if (i + 1) % print_interval == 0: \n",
    "                print('bkg Iteration {} Complete'.format(i + 1))\n",
    "\n",
    "            bkg_print_loss = loss.detach().cpu().numpy()\n",
    "            cur_losses.append(bkg_print_loss) \n",
    "            print('Loss: ', bkg_print_loss)\n",
    "\n",
    "            if bkg_print_loss < bkg_min_loss: \n",
    "                patience_count = 0\n",
    "                bkg_best_flow = bkg_flow\n",
    "                if save_models: \n",
    "                    torch.save(bkg_flow, \"pure_flows/bkg_%s.pt\" % (filename))\n",
    "                bkg_min_loss = bkg_print_loss\n",
    "                if (i + 1) % print_interval == 0: \n",
    "                    print('SAVING MODEL')\n",
    "            else: \n",
    "                patience_count += 1\n",
    "                if (i + 1) % print_interval == 0: \n",
    "                    print('NOT SAVING MODEL (PATIENCE = %s)' % patience_count)\n",
    "                if patience_count == 10: \n",
    "                    terminate = True\n",
    "                    break\n",
    "\n",
    "            bkg_tock = time.time()\n",
    "\n",
    "            if (i + 1) % print_interval == 0: \n",
    "                print('Time: ', bkg_tock - bkg_tick)\n",
    "                print('------------------------------------')\n",
    "                !nvidia-smi\n",
    "\n",
    "            #plt.contourf(xgrid.numpy(), ygrid.numpy(), bkg_zgrid.numpy())\n",
    "            #plt.title('iteration {}'.format(i + 1))\n",
    "            #plt.show()\n",
    "\n",
    "            #plt.hist(bkg_zgrid, bins=30, histtype='step')\n",
    "            #plt.title('iteration {}'.format(i + 1))\n",
    "            #plt.show()\n",
    "            \n",
    "    if terminate: \n",
    "        break\n",
    "\n",
    "bkg_loss_dict[hidden_features] = float(bkg_min_loss)\n",
    "bkg_flow_list.append(bkg_best_flow)\n",
    "\n",
    "print('------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cur_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Bkg-trained loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process sig samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 35\n",
    "\n",
    "Mjj_cut = 1200\n",
    "pt_cut = 550\n",
    "eta_cut = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_data, sig_unnorm_data, sig_masses = LAPS_train(sample_type = 'wprimesig', num_batches = num_batches, inp_meanstd = (bkg_mean, bkg_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sig_masses.shape)\n",
    "\n",
    "plt.hist(sig_masses, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureSig = torch.tensor(sig_data)\n",
    "total_PureSig_selection = total_PureSig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureSig_selection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 100 * num_batches\n",
    "sigAE_train_iterator = utils.DataLoader(total_PureSig_selection, batch_size=bs, shuffle=True) \n",
    "sigAE_test_iterator = utils.DataLoader(total_PureSig_selection, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the sig-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 14\n",
    "hidden_features = 56\n",
    "\n",
    "num_layers = 4\n",
    "num_blocks_per_layer = 4\n",
    "#num_iter = 10000\n",
    "num_iter = 1000\n",
    "print_interval = 20\n",
    "\n",
    "#Current flow_type options: 'MAF', 'NSQUAD' (neural spline quadratic), 'NSRATQUAD' (neural spline rational quadratic)\n",
    "flow_type = 'NSQUAD'\n",
    "\n",
    "save_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device =\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sig_data[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_loss_dict = dict()\n",
    "sig_flow_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Pure_NF_%s_k%s_hf%s_nbpl%s' % (flow_type, num_layers, hidden_features, num_blocks_per_layer)\n",
    "\n",
    "print(\"FCNN Hidden Layer Width: \", hidden_features)\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "sig_base_dist = StandardNormal(shape=[num_features])\n",
    "\n",
    "sig_transforms = []\n",
    "for _ in range(num_layers):\n",
    "    sig_transforms.append(ReversePermutation(features=num_features))\n",
    "    if flow_type == 'MAF': \n",
    "        sig_transforms.append(MaskedAffineAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features))\n",
    "    elif flow_type == 'NSQUAD': \n",
    "        sig_transforms.append(MaskedPiecewiseQuadraticAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "    elif flow_type == 'NSRATQUAD': \n",
    "        sig_transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(features=num_features, \n",
    "                                                          hidden_features=hidden_features, tail_bound = 3.0, tails='linear'))\n",
    "\n",
    "sig_transform = CompositeTransform(sig_transforms)\n",
    "\n",
    "sig_flow = Flow(sig_transform, sig_base_dist)\n",
    "#print(sig_flow)\n",
    "\n",
    "sig_optimizer = optim.Adam(sig_flow.parameters())\n",
    "\n",
    "sig_tick = time.time()\n",
    "\n",
    "sig_min_loss = 999999\n",
    "sig_best_flow = None\n",
    "\n",
    "cur_losses = []\n",
    "patience_count = 0\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    terminate = False\n",
    "\n",
    "    for batch_idx, x in enumerate(sigAE_train_iterator):\n",
    "\n",
    "        #x, y = datasets.make_moons(1024, noise=.1)\n",
    "        #x = sig_tr_data\n",
    "        #x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        sig_optimizer.zero_grad()\n",
    "        loss = -sig_flow.log_prob(inputs=x)[0].mean()\n",
    "        loss.backward()\n",
    "        sig_optimizer.step()\n",
    "\n",
    "        if batch_idx == len(sigAE_train_iterator) - 1 :\n",
    "\n",
    "            xline = torch.linspace(-2, 2)\n",
    "            yline = torch.linspace(-2, 2)\n",
    "            xgrid, ygrid = torch.meshgrid(xline, yline)\n",
    "            xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #sig_zgrid = sig_flow.log_prob(xyinput)[0].exp().reshape(100, 100)\n",
    "                sig_zgrid = -sig_flow.log_prob(x)[0].cpu().numpy()\n",
    "\n",
    "                #print(sig_zgrid.shape)\n",
    "                #sig_zgrid = sig_zgrid[sig_zgrid < 10]\n",
    "                #print(sig_zgrid.shape)\n",
    "                #print(sig_zgrid[:5])\n",
    "                \n",
    "            if (i + 1) % print_interval == 0: \n",
    "                print('sig Iteration {} Complete'.format(i + 1))\n",
    "\n",
    "            sig_print_loss = loss.detach().cpu().numpy()\n",
    "            cur_losses.append(sig_print_loss) \n",
    "            print('Loss: ', sig_print_loss)\n",
    "\n",
    "            if sig_print_loss < sig_min_loss: \n",
    "                patience_count = 0\n",
    "                sig_best_flow = sig_flow\n",
    "                if save_models: \n",
    "                    torch.save(sig_flow, \"pure_flows/sig_%s.pt\" % (filename))\n",
    "                sig_min_loss = sig_print_loss\n",
    "                if (i + 1) % print_interval == 0: \n",
    "                    print('SAVING MODEL')\n",
    "            else: \n",
    "                patience_count += 1\n",
    "                if (i + 1) % print_interval == 0: \n",
    "                    print('NOT SAVING MODEL (PATIENCE = %s)' % patience_count)\n",
    "                if patience_count == 10: \n",
    "                    terminate = True\n",
    "                    break\n",
    "\n",
    "            sig_tock = time.time()\n",
    "\n",
    "            if (i + 1) % print_interval == 0: \n",
    "                print('Time: ', sig_tock - sig_tick)\n",
    "                print('------------------------------------')\n",
    "                !nvidia-smi\n",
    "\n",
    "            #plt.contourf(xgrid.numpy(), ygrid.numpy(), sig_zgrid.numpy())\n",
    "            #plt.title('iteration {}'.format(i + 1))\n",
    "            #plt.show()\n",
    "\n",
    "            #plt.hist(sig_zgrid, bins=30, histtype='step')\n",
    "            #plt.title('iteration {}'.format(i + 1))\n",
    "            #plt.show()\n",
    "            \n",
    "    if terminate: \n",
    "        break\n",
    "\n",
    "sig_loss_dict[hidden_features] = float(sig_min_loss)\n",
    "sig_flow_list.append(sig_best_flow)\n",
    "\n",
    "print('------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cur_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Sig-trained Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bkg-trained post-training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_test = torch.tensor(bkg_data)\n",
    "sig_test = torch.tensor(sig_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_fpr_bkgtr(sigloss,bkgloss):\n",
    "    bins = np.linspace(0,100,10001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for cut in bins:\n",
    "        tpr.append(np.where(sigloss>cut)[0].shape[0]/len(sigloss))\n",
    "        fpr.append(np.where(bkgloss>cut)[0].shape[0]/len(bkgloss))\n",
    "\n",
    "    return tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bkg_flow_list = []\n",
    "\n",
    "new_bkg_loss_dict = dict()\n",
    "\n",
    "filename = 'Pure_NF_%s_k%s_hf%s_nbpl%s' % (flow_type, num_layers, hidden_features, num_blocks_per_layer)\n",
    "\n",
    "new_bkg_flow = torch.load(\"pure_flows/bkg_%s.pt\" % (filename))\n",
    "\n",
    "new_bkg_flow_list.append(new_bkg_flow)\n",
    "\n",
    "new_bkg_loss = -new_bkg_flow.log_prob(bkg_data)[0].mean().detach().cpu().numpy()\n",
    "new_bkg_loss_dict[hidden_features] = new_bkg_loss\n",
    "\n",
    "print(bkg_loss_dict)\n",
    "print(new_bkg_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_tr = int(bkg_data.shape[0])\n",
    "\n",
    "with torch.no_grad(): \n",
    "    bkg_samples = new_bkg_flow_list[-1].sample(num_samples_tr).detach().cpu().numpy()\n",
    "#bkg_samples = new_bkg_flow_list[0].sample(num_samples_tr).detach().cpu().numpy()\n",
    "print(bkg_samples.shape)\n",
    "bkg_samples_bad_indices = np.argwhere((bkg_samples < -10) | (bkg_samples > 10))[:,0]\n",
    "print(bkg_samples_bad_indices[:25])\n",
    "new_bkg_samples = np.delete(bkg_samples, bkg_samples_bad_indices, axis = 0)\n",
    "\n",
    "print(new_bkg_samples.shape)\n",
    "\n",
    "print(\"Hidden Features: \", hidden_features)\n",
    "\n",
    "plot_titles = [r'$M_{j1}$', r'Jet 1 $\\tau_{21}$', r'Jet 1 $\\tau_{32}$', r'Jet 1 $\\tau_{43}$', r'Jet 1 $\\tau_s$', r'Jet 1 $P_b$', r'Jet 1 $n_{pf}$', \n",
    "              r'$M_{j2}$', r'Jet 2 $\\tau_{21}$', r'Jet 2 $\\tau_{32}$', r'Jet 2 $\\tau_{43}$', r'Jet 2 $\\tau_s$', r'Jet 2 $P_b$', r'Jet 2 $n_{pf}$',]\n",
    "\n",
    "for index in range(num_features): \n",
    "    n, bins, patches = plt.hist(bkg_data[:, index], bins=50, histtype='step', label='Input distribution')\n",
    "    plt.hist(new_bkg_samples[:, index], bins=bins, histtype='step', label='NF estimated density')\n",
    "    if index % 7 == 4: \n",
    "        plt.legend(loc=(1.04,0.85))\n",
    "    plt.title(plot_titles[index])\n",
    "    plt.show()\n",
    "    \n",
    "#plt.hist outputs binning, pass that as input to make binning the same for two hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_bkgloss = -new_bkg_flow.log_prob(bkg_data)[0].detach().cpu().numpy()\n",
    "bkgtr_sigloss = -new_bkg_flow.log_prob(sig_data)[0].detach().cpu().numpy()\n",
    "print(bkgtr_bkgloss.shape)\n",
    "print(bkgtr_sigloss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr = get_tpr_fpr_bkgtr(bkgtr_sigloss, bkgtr_bkgloss)\n",
    "tpr_np, fpr_np = np.array(tpr), np.array(fpr)\n",
    "\n",
    "nonzero_idx = np.nonzero(fpr_np)\n",
    "\n",
    "tpr_inverse = tpr_np[nonzero_idx]\n",
    "fpr_inverse = 1/fpr_np[nonzero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tpr_inverse,fpr_inverse)\n",
    "plt.xlabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "plt.ylabel(r'$1/\\epsilon_{bkg}$',fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.title('Bkg-trained Pure NF Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_auc = metrics.auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel(r'$\\epsilon_{bkg}$',fontsize=15)\n",
    "plt.ylabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "plt.title('Bkg-trained Pure NF Model (AUC = %s)' % round(bkgtr_auc,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sig-trained post-training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_fpr_sigtr(sigloss,bkgloss):\n",
    "    bins = np.linspace(0,100,10001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for cut in bins:\n",
    "        tpr.append(np.where(sigloss<cut)[0].shape[0]/len(sigloss))\n",
    "        fpr.append(np.where(bkgloss<cut)[0].shape[0]/len(bkgloss))\n",
    "\n",
    "    return tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sig_flow_list = []\n",
    "\n",
    "new_sig_loss_dict = dict()\n",
    "\n",
    "filename = 'Pure_NF_%s_k%s_hf%s_nbpl%s' % (flow_type, num_layers, hidden_features, num_blocks_per_layer)\n",
    "\n",
    "new_sig_flow = torch.load(\"pure_flows/sig_%s.pt\" % (filename))\n",
    "\n",
    "new_sig_flow_list.append(new_sig_flow)\n",
    "\n",
    "new_sig_loss = -new_sig_flow.log_prob(sig_data)[0].mean().detach().cpu().numpy()\n",
    "new_sig_loss_dict[hidden_features] = new_sig_loss\n",
    "\n",
    "print(sig_loss_dict)\n",
    "print(new_sig_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_tr = int(sig_data.shape[0])\n",
    "\n",
    "with torch.no_grad(): \n",
    "    sig_samples = new_sig_flow_list[-1].sample(num_samples_tr).detach().cpu().numpy()\n",
    "#sig_samples = new_sig_flow_list[0].sample(num_samples_tr).detach().cpu().numpy()\n",
    "print(sig_samples.shape)\n",
    "sig_samples_bad_indices = np.argwhere((sig_samples < -10) | (sig_samples > 10))[:,0]\n",
    "print(sig_samples_bad_indices[:25])\n",
    "new_sig_samples = np.delete(sig_samples, sig_samples_bad_indices, axis = 0)\n",
    "\n",
    "print(new_sig_samples.shape)\n",
    "\n",
    "print(\"Hidden Features: \", hidden_features)\n",
    "\n",
    "plot_titles = [r'$M_{j1}$', r'Jet 1 $\\tau_{21}$', r'Jet 1 $\\tau_{32}$', r'Jet 1 $\\tau_{43}$', r'Jet 1 $\\tau_s$', r'Jet 1 $P_b$', r'Jet 1 $n_{pf}$', \n",
    "              r'$M_{j2}$', r'Jet 2 $\\tau_{21}$', r'Jet 2 $\\tau_{32}$', r'Jet 2 $\\tau_{43}$', r'Jet 2 $\\tau_s$', r'Jet 2 $P_b$', r'Jet 2 $n_{pf}$',]\n",
    "\n",
    "for index in range(num_features): \n",
    "    n, bins, patches = plt.hist(sig_data[:, index], bins=50, histtype='step', label='Input distribution')\n",
    "    plt.hist(new_sig_samples[:, index], bins=bins, histtype='step', label='NF estimated density')\n",
    "    if index % 7 == 4: \n",
    "        plt.legend(loc=(1.04,0.85))\n",
    "    plt.title(plot_titles[index])\n",
    "    plt.show()\n",
    "    \n",
    "#plt.hist outputs binning, pass that as input to make binning the same for two hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_bkgloss = -new_sig_flow.log_prob(bkg_data)[0].detach().cpu().numpy()\n",
    "sigtr_sigloss = -new_sig_flow.log_prob(sig_data)[0].detach().cpu().numpy()\n",
    "print(sigtr_bkgloss.shape)\n",
    "print(sigtr_sigloss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr = get_tpr_fpr_sigtr(sigtr_sigloss, sigtr_bkgloss)\n",
    "tpr_np, fpr_np = np.array(tpr), np.array(fpr)\n",
    "\n",
    "nonzero_idx = np.nonzero(fpr_np)\n",
    "\n",
    "tpr_inverse = tpr_np[nonzero_idx]\n",
    "fpr_inverse = 1/fpr_np[nonzero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tpr_inverse,fpr_inverse)\n",
    "plt.xlabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "plt.ylabel(r'$1/\\epsilon_{bkg}$',fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.title('Sig-trained Pure NF Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_auc = metrics.auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel(r'$\\epsilon_{bkg}$',fontsize=15)\n",
    "plt.ylabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "plt.title('Sig-trained Pure NF Model (AUC = %s)' % round(sigtr_auc,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D ROC Curves (Still in Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hull_coordinates(scan_tpr, scan_fpr):\n",
    "    points = np.array([scan_fpr,scan_tpr])\n",
    "    points = points.transpose()\n",
    "    hull = ConvexHull(points)\n",
    "    fpr = np.array(points[hull.vertices[:],0])\n",
    "    tpr = np.array(points[hull.vertices[:],1])\n",
    "    nonzero_idx = np.nonzero(fpr)\n",
    "    fpr = fpr[nonzero_idx]\n",
    "    tpr = tpr[nonzero_idx]\n",
    "    return tpr, fpr \n",
    "\n",
    "def get_tpr_fpr_2d(sigae_sigloss,sigae_bkgloss,bkgae_sigloss,bkgae_bkgloss):\n",
    "    bins_sigae = np.arange(0,30,0.1)\n",
    "    bins_bkgae = np.arange(0,30,0.1)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for sigcut in bins_sigae:\n",
    "        for bkgcut in bins_bkgae:\n",
    "            tpr_t = np.where((sigae_sigloss<sigcut)&(bkgae_sigloss>bkgcut))[0].shape[0]/len(sigae_sigloss)\n",
    "            fpr_t = np.where((sigae_bkgloss<sigcut)&(bkgae_bkgloss>bkgcut))[0].shape[0]/len(sigae_bkgloss)\n",
    "            tpr.append(tpr_t)\n",
    "            fpr.append(fpr_t)\n",
    "            \n",
    "    tpr, fpr = get_hull_coordinates(tpr, fpr)\n",
    "                \n",
    "\n",
    "    return tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr2dlist = []\n",
    "fpr2dlist = []\n",
    "tpr2dlist_forinverse = []\n",
    "fpr2dinverselist = []\n",
    "\n",
    "SSLs = sigtr_sigloss\n",
    "SBLs = sigtr_bkgloss\n",
    "BSLs = bkgtr_sigloss\n",
    "BBLs = bkgtr_bkgloss\n",
    "\n",
    "print(SSLs[:20])\n",
    "print(SBLs[:20])\n",
    "print(BSLs[:20])\n",
    "print(BBLs[:20])\n",
    "\n",
    "tpr2d, fpr2d = get_tpr_fpr_2d(SSLs, SBLs, BSLs, BBLs)\n",
    "print(tpr2d.shape)\n",
    "print(fpr2d.shape)\n",
    "\n",
    "tpr2d_np, fpr2d_np = np.array(tpr2d), np.array(fpr2d)\n",
    "\n",
    "nonzero_idx = np.nonzero(fpr2d_np)\n",
    "\n",
    "tpr2d_inverse = tpr2d_np[nonzero_idx]\n",
    "fpr2d_inverse = 1/fpr2d_np[nonzero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tpr2d_inverse,fpr2d_inverse)\n",
    "plt.xlabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "plt.ylabel(r'$1/\\epsilon_{bkg}$',fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.title('2D ROC Curve (Pure NF Models)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc2d = metrics.auc(fpr2d,tpr2d)\n",
    "plt.plot(fpr2d,tpr2d)\n",
    "plt.xlabel(r'$\\epsilon_{bkg}$',fontsize=15)\n",
    "plt.ylabel(r'$\\epsilon_{sig}$',fontsize=15)\n",
    "plt.title('2D ROC Curve (Pure NF Models)' % auc2d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUAK spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bkgtr_bkgloss = np.append(bkgtr_bkgloss, np.array([0,]))\n",
    "temp_sigtr_bkgloss = np.append(sigtr_bkgloss, np.array([0,]))\n",
    "temp_bkgtr_sigloss = np.append(bkgtr_sigloss, np.array([0,]))\n",
    "temp_sigtr_sigloss = np.append(sigtr_sigloss, np.array([0,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multidim_cumsum(a):\n",
    "    out = a.cumsum(-1)\n",
    "    for i in range(2,a.ndim+1):\n",
    "        np.cumsum(out, axis=-i, out=out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cdf(inp_h_bkg, inp_h_sig, inp_X, inp_Y, inp_plot_lower_bound = None, inp_plot_upper_bound = None, num_bins = 50, save_cdfs = False): \n",
    "\n",
    "    bkg_cumulative_counts = multidim_cumsum(inp_h_bkg)\n",
    "    sig_cumulative_counts = multidim_cumsum(inp_h_sig)\n",
    "\n",
    "    cumulative_counts = sig_cumulative_counts/np.sqrt(bkg_cumulative_counts)\n",
    "    flattened_cumulative_counts = cumulative_counts.flatten()\n",
    "\n",
    "    h_bkg_sig, _, _, _ = plt.hist2d(inp_X, inp_Y, weights=flattened_cumulative_counts, cmap = plt.cm.jet, bins=num_bins)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"bkg-Trained Model Loss\")\n",
    "    plt.ylabel(\"sig-Trained Model Loss\")\n",
    "    plt.title(\"S/sqrt(B) 2D Loss Histogram (CDF)\")\n",
    "    \n",
    "    if inp_plot_lower_bound is not None and inp_plot_upper_bound is not None: \n",
    "        plt.xlim(inp_plot_lower_bound, inp_plot_upper_bound)\n",
    "        plt.ylim(inp_plot_lower_bound, inp_plot_upper_bound)\n",
    "    \n",
    "    if save_cdfs: \n",
    "        print(filename)\n",
    "        plt.savefig('cdfs/%s.png' % (filename))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_loss_only = True\n",
    "x_bad_loss_cutoff = 50\n",
    "y_bad_loss_cutoff = 50\n",
    "    \n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "num_bins = 2000\n",
    "\n",
    "h_bkg, bkg_xedges, bkg_yedges, _ = plt.hist2d(temp_bkgtr_bkgloss, temp_sigtr_bkgloss, cmap = plt.cm.jet, bins=num_bins)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"BKG-Trained Model Loss\")\n",
    "plt.ylabel(\"SIG-Trained Model Loss\")\n",
    "plt.title(\"BKG Training Data 2D Loss Histogram (PDF)\")\n",
    "plt.xlim(0, x_bad_loss_cutoff)\n",
    "plt.ylim(0, y_bad_loss_cutoff)\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "num_bins = 250\n",
    "\n",
    "h_sig, sig_xedges, sig_yedges, _ = plt.hist2d(temp_bkgtr_sigloss, temp_sigtr_sigloss, cmap = plt.cm.jet, bins=num_bins)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"BKG-Trained Model Loss\")\n",
    "plt.ylabel(\"SIG-Trained Model Loss\")\n",
    "plt.title(\"SIG Training Data 2D Loss Histogram (PDF)\")\n",
    "plt.xlim(0, x_bad_loss_cutoff)\n",
    "plt.ylim(0, y_bad_loss_cutoff)\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "num_bins = 1000\n",
    "save_cdfs = False\n",
    "\n",
    "x_lowest_bin_center = np.min(bkg_xedges) + (bkg_xedges[1]-bkg_xedges[0]) / 2\n",
    "\n",
    "x_highest_bin_center = np.max(bkg_xedges) - (bkg_xedges[1]-bkg_xedges[0]) / 2\n",
    "x_bin_step = (bkg_xedges[1]-bkg_xedges[0])\n",
    "\n",
    "y_lowest_bin_center = np.min(bkg_yedges) + (bkg_yedges[1]-bkg_yedges[0]) / 2\n",
    "y_highest_bin_center = np.max(bkg_yedges) - (bkg_yedges[1]-bkg_yedges[0]) / 2\n",
    "y_bin_step = (bkg_yedges[1]-bkg_yedges[0])\n",
    "\n",
    "h_bkg, bkg_xedges, bkg_yedges, _ = plt.hist2d(temp_bkgtr_bkgloss, temp_sigtr_bkgloss, cmap = plt.cm.jet, bins=num_bins)\n",
    "h_sig, sig_xedges, sig_yedges, _ = plt.hist2d(temp_bkgtr_sigloss, temp_sigtr_sigloss, cmap = plt.cm.jet, bins=num_bins)\n",
    "#print(h_bkg)\n",
    "#print(h_skg)\n",
    "\n",
    "counts = h_sig/np.sqrt(h_bkg)\n",
    "flattened_counts = counts.flatten()\n",
    "\n",
    "print(num_bins*x_bin_step)\n",
    "\n",
    "X,Y = np.mgrid[x_lowest_bin_center:x_highest_bin_center+x_bin_step:x_bin_step, y_lowest_bin_center:y_highest_bin_center+y_bin_step:y_bin_step]\n",
    "X = X[:num_bins, :num_bins]\n",
    "Y = Y[:num_bins, :num_bins]\n",
    "X = X.flatten()\n",
    "Y = Y.flatten()\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(counts.shape)\n",
    "print(flattened_counts.shape)\n",
    "\n",
    "h_bkg_sig, _, _, _ = plt.hist2d(X, Y, weights=flattened_counts, cmap = plt.cm.jet, bins=num_bins)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"BKG-Trained Model Loss\")\n",
    "plt.ylabel(\"SIG-Trained Model Loss\")\n",
    "plt.title(\"S/sqrt(B) 2D Loss Histogram (PDF)\")\n",
    "plt.xlim(0, x_bad_loss_cutoff)\n",
    "plt.ylim(0, y_bad_loss_cutoff)\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "plot_cdf(h_bkg, h_sig, X, Y, 0, x_bad_loss_cutoff, num_bins=num_bins, save_cdfs=save_cdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bump Hunt CSV File Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bkg_batches = 2\n",
    "num_sig_batches = 1\n",
    "\n",
    "sig_sampling_percentage = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_data, bkg_unnorm_data, bkg_masses = LAPS_test(sample_type = 'qcdbkg', num_batches = num_bkg_batches)\n",
    "\n",
    "bkg_mean = np.mean(bkg_unnorm_data, axis=0)\n",
    "bkg_std = np.std(bkg_unnorm_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_data, sig_unnorm_data, sig_masses = LAPS_test(sample_type = 'wprimesig', num_batches = num_sig_batches, inp_meanstd = (bkg_mean, bkg_std))\n",
    "\n",
    "sig_num_samples = sig_data.shape[0]\n",
    "sampling_indices = np.random.randint(sig_num_samples, size = int(sig_sampling_percentage * sig_num_samples / 100))\n",
    "sig_data = sig_data[sampling_indices, :]\n",
    "sig_unnorm_data = sig_unnorm_data[sampling_indices, :]\n",
    "sig_masses = sig_masses[sampling_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bkg_samples = bkg_data.shape[0]\n",
    "num_sig_samples = sig_data.shape[0]\n",
    "sig_sample_percentage = 100 * num_sig_samples / (num_bkg_samples + num_sig_samples)\n",
    "\n",
    "bkg_data_WL = np.concatenate((bkg_data, np.zeros((num_bkg_samples,1), dtype='float32')), axis=1)\n",
    "sig_data_WL = np.concatenate((sig_data, np.ones((num_sig_samples,1), dtype='float32')), axis=1)\n",
    "\n",
    "test_data = np.concatenate((bkg_data_WL, sig_data_WL), axis=0)\n",
    "test_masses = np.concatenate((bkg_masses, sig_masses), axis=0)\n",
    "#test_data = bkg_data_WL\n",
    "#test_masses = bkg_masses\n",
    "test_data_WM = np.concatenate((test_masses, test_data), axis=1)\n",
    "\n",
    "np.random.shuffle(test_data_WM)\n",
    "\n",
    "test_masses = test_data_WM[:,0]\n",
    "test_labels = test_data_WM[:,-1]\n",
    "test_data = test_data_WM[:,1:-1]\n",
    "\n",
    "num_test_samples = test_data.shape[0]\n",
    "\n",
    "print(\"Num bkg samples: \", num_bkg_samples)\n",
    "print(\"Num sig samples: \", num_sig_samples)\n",
    "print(\"Num test samples: \", num_test_samples)\n",
    "print(\"Signal Percentage: \", sig_sample_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mass = pd.DataFrame(np.ndarray.tolist(test_masses))\n",
    "df_mass.to_csv('csv_files/test_masses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Pure_NF_%s_k%s_hf%s_nbpl%s' % (flow_type, num_layers, hidden_features, num_blocks_per_layer)\n",
    "bkg_model = torch.load(\"pure_flows/bkg_%s.pt\" % (filename))\n",
    "sig_model = torch.load(\"pure_flows/sig_%s.pt\" % (filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_loss_indices = np.where(test_labels==0)\n",
    "sig_loss_indices = np.where(test_labels==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_test_losses = -bkg_model.log_prob(test_data)[0].detach().cpu().numpy()\n",
    "print(bkgtr_test_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_bkg_losses = bkgtr_test_losses[bkg_loss_indices]\n",
    "print(bkgtr_bkg_losses.shape)\n",
    "\n",
    "bkgtr_sig_losses = bkgtr_test_losses[sig_loss_indices]\n",
    "print(bkgtr_sig_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bkgloss = pd.DataFrame(np.ndarray.tolist(bkgtr_test_losses))\n",
    "df_bkgloss.to_csv('csv_files/bkgtr_test_losses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_test_losses = -sig_model.log_prob(test_data)[0].detach().cpu().numpy()\n",
    "print(sigtr_test_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_bkg_losses = sigtr_test_losses[bkg_loss_indices]\n",
    "print(sigtr_bkg_losses.shape)\n",
    "\n",
    "sigtr_sig_losses = sigtr_test_losses[sig_loss_indices]\n",
    "print(sigtr_sig_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sigloss = pd.DataFrame(np.ndarray.tolist(sigtr_test_losses))\n",
    "df_sigloss.to_csv('csv_files/sigtr_test_losses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgloss_cut = 15\n",
    "low_bkgloss_indices = np.where(bkgtr_test_losses > bkgloss_cut)[0]\n",
    "for index in range(2,8):  \n",
    "    \n",
    "    sigloss_cut = 5*index\n",
    "    low_sigloss_indices = np.where(sigtr_test_losses < sigloss_cut)[0]\n",
    "    low_loss_indices = np.intersect1d(low_bkgloss_indices, low_sigloss_indices)\n",
    "    low_loss_test_masses = test_masses[low_loss_indices]\n",
    "    low_loss_bkg_masses = test_masses[np.intersect1d(low_loss_indices, bkg_loss_indices)]\n",
    "    \n",
    "    n, bins, patches = plt.hist(low_loss_test_masses, bins=50, histtype = 'step', label = 'bkg + sig')\n",
    "    plt.hist(low_loss_bkg_masses, bins = bins, histtype = 'step', label = 'bkg only')\n",
    "    \n",
    "    plt.xlabel('bkgloss > %s, sigloss < %s' % (bkgloss_cut, sigloss_cut))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bkgtr_bkg_losses, sigtr_bkg_losses, s=2, label = 'QCD bkg samples')\n",
    "plt.scatter(bkgtr_sig_losses, sigtr_sig_losses, s=2, label = r'W$\\rightarrow$WZ sig samples')\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0,100)\n",
    "plt.xlabel(\"bkg-Trained Model Loss\")\n",
    "plt.ylabel(\"sig-Trained Model Loss\")\n",
    "plt.title(\"Testing Data QUAK Space\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and processs graviton samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graviton_data, graviton_unnorm_data, graviton_masses = LAPS_test(sample_type = 'graviton', num_batches = num_batches, inp_meanstd = (bkg_mean, bkg_std))\n",
    "print(graviton_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_graviton_losses = -bkg_model.log_prob(graviton_data)[0].detach().cpu().numpy()\n",
    "print(bkgtr_graviton_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_graviton_losses = -sig_model.log_prob(graviton_data)[0].detach().cpu().numpy()\n",
    "print(sigtr_graviton_losses.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process Wkk samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wkk_data, Wkk_unnorm_data, Wkk_masses = LAPS_test(sample_type = 'Wkk', num_batches = num_batches, inp_meanstd = (bkg_mean, bkg_std))\n",
    "print(Wkk_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgtr_Wkk_losses = -bkg_model.log_prob(Wkk_data)[0].detach().cpu().numpy()\n",
    "print(bkgtr_Wkk_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigtr_Wkk_losses = -sig_model.log_prob(Wkk_data)[0].detach().cpu().numpy()\n",
    "print(sigtr_Wkk_losses.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master QUAK Space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bkgtr_bkg_losses, sigtr_bkg_losses, s=2, label = 'QCD bkg samples')\n",
    "plt.scatter(bkgtr_sig_losses, sigtr_sig_losses, s=2, label = 'W\\' sig samples')\n",
    "plt.scatter(bkgtr_graviton_losses, sigtr_graviton_losses, s=2, label = 'Graviton sig samples')\n",
    "plt.scatter(bkgtr_Wkk_losses, sigtr_Wkk_losses, s=2, label = 'Wkk sig samples')\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0,100)\n",
    "plt.xlabel(\"bkg-Trained Model Loss\")\n",
    "plt.ylabel(\"sig-Trained Model Loss\")\n",
    "plt.title(\"Testing Data QUAK Space\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
