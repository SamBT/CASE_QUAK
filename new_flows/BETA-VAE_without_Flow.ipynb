{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import torch.utils.data as utils\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_rnd = pd.read_hdf(\"/data/t3home000/spark/LHCOlympics_previous/LHC-Olympics/Code/Nsubjettiness_mjj.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
       "       '13', '14', '15'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_rnd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98.677270</td>\n",
       "      <td>0.528903</td>\n",
       "      <td>0.788281</td>\n",
       "      <td>0.904471</td>\n",
       "      <td>4.241889</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1285.895950</td>\n",
       "      <td>53.519023</td>\n",
       "      <td>0.668562</td>\n",
       "      <td>0.735745</td>\n",
       "      <td>0.755674</td>\n",
       "      <td>1.895988</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1282.286017</td>\n",
       "      <td>2577.571899</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>584.595432</td>\n",
       "      <td>0.345626</td>\n",
       "      <td>0.463461</td>\n",
       "      <td>0.865982</td>\n",
       "      <td>1.069972</td>\n",
       "      <td>320.0</td>\n",
       "      <td>1334.493332</td>\n",
       "      <td>405.034096</td>\n",
       "      <td>0.264362</td>\n",
       "      <td>0.793461</td>\n",
       "      <td>0.830032</td>\n",
       "      <td>1.377217</td>\n",
       "      <td>348.0</td>\n",
       "      <td>1306.137883</td>\n",
       "      <td>3807.507389</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159.597526</td>\n",
       "      <td>0.677692</td>\n",
       "      <td>0.690707</td>\n",
       "      <td>0.695322</td>\n",
       "      <td>1.310040</td>\n",
       "      <td>332.0</td>\n",
       "      <td>678.557182</td>\n",
       "      <td>113.768840</td>\n",
       "      <td>0.713481</td>\n",
       "      <td>0.922610</td>\n",
       "      <td>0.782783</td>\n",
       "      <td>1.887494</td>\n",
       "      <td>236.0</td>\n",
       "      <td>1072.462085</td>\n",
       "      <td>1710.965414</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>515.237299</td>\n",
       "      <td>0.091038</td>\n",
       "      <td>0.784454</td>\n",
       "      <td>0.860716</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>248.0</td>\n",
       "      <td>1284.020224</td>\n",
       "      <td>161.648798</td>\n",
       "      <td>0.727507</td>\n",
       "      <td>0.719564</td>\n",
       "      <td>0.870109</td>\n",
       "      <td>1.997360</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1217.031950</td>\n",
       "      <td>2603.379037</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142.420213</td>\n",
       "      <td>0.507714</td>\n",
       "      <td>0.522686</td>\n",
       "      <td>0.904070</td>\n",
       "      <td>1.853319</td>\n",
       "      <td>220.0</td>\n",
       "      <td>1087.658980</td>\n",
       "      <td>105.721163</td>\n",
       "      <td>0.344534</td>\n",
       "      <td>0.614579</td>\n",
       "      <td>0.863765</td>\n",
       "      <td>1.113248</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1205.343324</td>\n",
       "      <td>3294.162200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4      5            6  \\\n",
       "0   98.677270  0.528903  0.788281  0.904471  4.241889  136.0  1285.895950   \n",
       "1  584.595432  0.345626  0.463461  0.865982  1.069972  320.0  1334.493332   \n",
       "2  159.597526  0.677692  0.690707  0.695322  1.310040  332.0   678.557182   \n",
       "3  515.237299  0.091038  0.784454  0.860716  1.102743  248.0  1284.020224   \n",
       "4  142.420213  0.507714  0.522686  0.904070  1.853319  220.0  1087.658980   \n",
       "\n",
       "            7         8         9        10        11     12           13  \\\n",
       "0   53.519023  0.668562  0.735745  0.755674  1.895988  128.0  1282.286017   \n",
       "1  405.034096  0.264362  0.793461  0.830032  1.377217  348.0  1306.137883   \n",
       "2  113.768840  0.713481  0.922610  0.782783  1.887494  236.0  1072.462085   \n",
       "3  161.648798  0.727507  0.719564  0.870109  1.997360  352.0  1217.031950   \n",
       "4  105.721163  0.344534  0.614579  0.863765  1.113248  204.0  1205.343324   \n",
       "\n",
       "            14   15  \n",
       "0  2577.571899  0.0  \n",
       "1  3807.507389  0.0  \n",
       "2  1710.965414  0.0  \n",
       "3  2603.379037  0.0  \n",
       "4  3294.162200  1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_rnd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'ROC':\n",
    "    dt = f_rnd.values\n",
    "else:\n",
    "    dt_PureBkg = f_PureBkg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [0,1,2,3,4,5,7,8,9,10,11,12]\n",
    "for i in index_list:\n",
    "    dt[:,i] = (dt[:,i]-np.mean(dt[:,i]))/np.std(dt[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.76387849e-01,  1.92399512e-01,  6.41817483e-01, ...,\n",
       "         1.28228602e+03,  2.57757190e+03,  0.00000000e+00],\n",
       "       [ 2.56368470e+00, -7.40919608e-01, -1.86301047e+00, ...,\n",
       "         1.30613788e+03,  3.80750739e+03,  0.00000000e+00],\n",
       "       [-5.32563884e-01,  9.50088429e-01, -1.10614099e-01, ...,\n",
       "         1.07246208e+03,  1.71096541e+03,  0.00000000e+00],\n",
       "       ...,\n",
       "       [-1.14348172e+00,  7.85252036e-01,  1.12078063e-01, ...,\n",
       "         1.41629773e+03,  3.23296780e+03,  0.00000000e+00],\n",
       "       [-7.63183147e-01, -5.63390513e-02, -6.01315795e-01, ...,\n",
       "         1.13146135e+03,  3.60117240e+03,  0.00000000e+00],\n",
       "       [-1.02491235e+00, -1.27034111e-01, -8.10810555e-01, ...,\n",
       "         1.07347082e+03,  2.49024909e+03,  0.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = dt[:,15]\n",
    "bkg_idx = np.where(idx==0)[0]\n",
    "signal_idx = np.where(idx==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0       1       2 ... 1099997 1099998 1099999]\n"
     ]
    }
   ],
   "source": [
    "print(bkg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt[bkg_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureBkg = torch.tensor(dt[bkg_idx])\n",
    "total_PureBkg_train_x_1 = total_PureBkg.t()[0:6].t()\n",
    "total_PureBkg_train_x_3 = total_PureBkg.t()[7:13].t()\n",
    "total_PureBkg_selection = torch.cat((total_PureBkg_train_x_1,total_PureBkg_train_x_3),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000000, 12])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_PureBkg_selection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(total_PureBkg_selection, [800000, 200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 500\n",
    "bkgAE_train_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs, shuffle=True)\n",
    "bkgAE_test_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "\n",
    "    '''\n",
    "    def __init__(self, z_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim: A integer indicating the size of input (in case of MNIST 28 * 28).\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            z_dim: A integer indicating the latent dimension.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(12, 10)\n",
    "        self.linear2 = nn.Linear(10, 5)\n",
    "        #self.linear3 = nn.Linear(30, 20)\n",
    "        #self.linear4 = nn.Linear(20, 10)\n",
    "        #self.linear5 = nn.Linear(10, 6)\n",
    "        hidden_dim = 5\n",
    "        self.mu = nn.Linear(hidden_dim, z_dim)\n",
    "        self.var = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        x = F.leaky_relu(self.linear1(x))\n",
    "        x = F.leaky_relu(self.linear2(x))\n",
    "        #x = F.leaky_relu(self.linear3(x))\n",
    "        #x = F.leaky_relu(self.linear4(x))\n",
    "        #x = F.leaky_relu(self.linear5(x))\n",
    "\n",
    "        #hidden = F.relu(self.linear(x))\n",
    "        # hidden is of shape [batch_size, hidden_dim]\n",
    "        \n",
    "        z_mu = self.mu(x)\n",
    "        # z_mu is of shape [batch_size, latent_dim]\n",
    "        z_var = self.var(x)\n",
    "        # z_var is of shape [batch_size, latent_dim]\n",
    "\n",
    "        return z_mu, z_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "\n",
    "    '''\n",
    "    def __init__(self, z_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            z_dim: A integer indicating the latent size.\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            output_dim: A integer indicating the output dimension (in case of MNIST it is 28 * 28)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(z_dim, 5)\n",
    "        self.linear2 = nn.Linear(5, 10)\n",
    "        #self.linear3 = nn.Linear(10, 20)\n",
    "        #self.linear4 = nn.Linear(20, 30)\n",
    "        #self.linear5 = nn.Linear(30, 48)\n",
    "        self.out = nn.Linear(10, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, latent_dim]\n",
    "        x = F.leaky_relu(self.linear1(x))\n",
    "        x = F.leaky_relu(self.linear2(x))\n",
    "        #x = F.leaky_relu(self.linear3(x))\n",
    "        #x = F.leaky_relu(self.linear4(x))\n",
    "        #x = F.leaky_relu(self.linear5(x))\n",
    "\n",
    "        #hidden = F.relu(self.linear(x))\n",
    "        # hidden is of shape [batch_size, hidden_dim]\n",
    "\n",
    "        predicted = torch.sigmoid(self.out(x))\n",
    "        # predicted is of shape [batch_size, output_dim]\n",
    "\n",
    "        return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, enc, dec):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_var = self.enc(x)\n",
    "\n",
    "        # sample from the distribution having latent parameters z_mu, z_var\n",
    "        # reparameterize\n",
    "        std = torch.exp(z_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        x_sample = eps.mul(std).add_(z_mu)\n",
    "\n",
    "        # decode\n",
    "        predicted = self.dec(x_sample)\n",
    "        return predicted, z_mu, z_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder = Encoder(3)\n",
    "\n",
    "# decoder\n",
    "decoder = Decoder(3)\n",
    "\n",
    "# vae\n",
    "model = VAE(encoder, decoder).to(device)\n",
    "\n",
    "# optimizer\n",
    "lr = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (enc): Encoder(\n",
      "    (linear1): Linear(in_features=12, out_features=10, bias=True)\n",
      "    (linear2): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (mu): Linear(in_features=5, out_features=3, bias=True)\n",
      "    (var): Linear(in_features=5, out_features=3, bias=True)\n",
      "  )\n",
      "  (dec): Decoder(\n",
      "    (linear1): Linear(in_features=3, out_features=5, bias=True)\n",
      "    (linear2): Linear(in_features=5, out_features=10, bias=True)\n",
      "    (out): Linear(in_features=10, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # set the train mode\n",
    "    model.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, x in enumerate(bkgAE_train_iterator):\n",
    "        # reshape the data into [batch_size, 784]\n",
    "        x = x.float().cuda()\n",
    "\n",
    "        # update the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        x_sample, z_mu, z_var = model(x)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = F.binary_cross_entropy(x_sample, x, size_average=False)\n",
    "\n",
    "        # kl divergence loss\n",
    "        kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
    "\n",
    "        # total loss\n",
    "        beta = .5\n",
    "        ##### if beta = 1 , then it is plain VAE\n",
    "        loss = recon_loss + beta * kl_loss\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # set the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # test loss for the data\n",
    "    test_loss = 0\n",
    "\n",
    "    # we don't need to track the gradients, since we are not updating the parameters during evaluation / testing\n",
    "    with torch.no_grad():\n",
    "        for i, x in enumerate(bkgAE_test_iterator):\n",
    "            # reshape the data\n",
    "            #x = x.view(-1, 28 * 28)\n",
    "            x = x.float().cuda()\n",
    "            # forward pass\n",
    "            x_sample, z_mu, z_var = model(x)\n",
    "\n",
    "            # reconstruction loss\n",
    "            recon_loss = F.binary_cross_entropy(x_sample, x, size_average=False)\n",
    "\n",
    "            # kl divergence loss\n",
    "            kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
    "\n",
    "            # total loss\n",
    "            loss = recon_loss + kl_loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/miniconda3/envs/myenv/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: -139.67, Test Loss: -135.79\n",
      "Saving model!\n",
      "Epoch 1, Train Loss: -139.57, Test Loss: -135.28\n",
      "Not saving model!\n",
      "Epoch 2, Train Loss: -139.41, Test Loss: -135.21\n",
      "Not saving model!\n",
      "Epoch 3, Train Loss: -139.26, Test Loss: -135.26\n",
      "Not saving model!\n",
      "Epoch 4, Train Loss: -139.14, Test Loss: -134.94\n",
      "Not saving model!\n",
      "Epoch 5, Train Loss: -138.97, Test Loss: -135.05\n",
      "Not saving model!\n",
      "Epoch 6, Train Loss: -138.77, Test Loss: -134.63\n",
      "Not saving model!\n",
      "Epoch 7, Train Loss: -138.73, Test Loss: -134.62\n",
      "Not saving model!\n",
      "Epoch 8, Train Loss: -138.72, Test Loss: -134.70\n",
      "Not saving model!\n",
      "Epoch 9, Train Loss: -138.66, Test Loss: -134.78\n",
      "Not saving model!\n",
      "Epoch 10, Train Loss: -138.87, Test Loss: -134.92\n",
      "Not saving model!\n",
      "Patience Limit Reached\n"
     ]
    }
   ],
   "source": [
    "best_test_loss = float('inf')\n",
    "\n",
    "for e in range(40):\n",
    "\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "\n",
    "    train_loss /= len(total_PureBkg_selection)\n",
    "    test_loss /= len(total_PureBkg_selection)\n",
    "\n",
    "    print(f'Epoch {e}, Train Loss: {train_loss:.2f}, Test Loss: {test_loss:.2f}')\n",
    "\n",
    "    if best_test_loss > test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        patience_counter = 1\n",
    "        print(\"Saving model!\")\n",
    "        if mode == 'ROC':\n",
    "            torch.save(model.state_dict(),\"/data/t3home000/spark/QUASAR/weights/bkg_BETAvae_Vanilla_RND_betadot5.h5\")\n",
    "        else:\n",
    "            torch.save(model.state_dict(), \"/data/t3home000/spark/QUASAR/weights/bkg_BETAvae_Vanilla_PureBkg.h5\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(\"Not saving model!\")\n",
    "\n",
    "    if patience_counter > 10:\n",
    "        print(\"Patience Limit Reached\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC\n"
     ]
    }
   ],
   "source": [
    "print(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/data/t3home000/spark/QUASAR/weights/bkg_BETAvae_Vanilla_RND_betadot5.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(dt, index_list):\n",
    "    print(dt.shape)\n",
    "    \n",
    "    #for i in index_list:\n",
    "    #    print(i)\n",
    "    #    dt[:,i] = (dt[:,i]-np.mean(dt[:,i]))/np.std(dt[:,i])\n",
    "  \n",
    "    \n",
    "    total_in = torch.tensor(dt)\n",
    "    total_in_train_x_1 = total_in.t()[0:6].t()\n",
    "    total_in_train_x_3 = total_in.t()[7:13].t()\n",
    "    total_in_selection = torch.cat((total_in_train_x_1,total_in_train_x_3),dim=1)\n",
    "    z_mu, z_var  = model.enc(total_in_selection.float().cuda())\n",
    "    std = torch.exp(z_var / 2)\n",
    "    eps = torch.randn_like(std)\n",
    "    x_sample = eps.mul(std).add_(z_mu)\n",
    "    decoded_bkg = model.dec(x_sample)\n",
    "    loss_bkg = torch.mean((decoded_bkg-total_in_selection.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "    #with torch.no_grad():\n",
    "        # reconstruction loss\n",
    "        #x_sample, z_mu, z_var = model(total_in_selection.float().cuda())\n",
    "        #recon_loss = F.binary_cross_entropy(x_sample, total_in_selection.float().cuda(), size_average=False, reduce=None)\n",
    "        \n",
    "\n",
    "        # kl divergence loss\n",
    "        #kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
    "\n",
    "        # total loss\n",
    "        #loss = recon_loss + kl_loss\n",
    "        #loss = torch.mean((model(total_in_selection.float().cuda())[0]- total_in_selection.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "    \n",
    "    return loss_bkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bkg = torch.tensor(dt[bkg_idx])\n",
    "data_signal = torch.tensor(dt[signal_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000000, 16])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bkg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x_1 = data_bkg.t()[0:6].t()\n",
    "data_train_x_2 = data_bkg.t()[7:13].t()\n",
    "data_test_bkg = torch.cat((data_train_x_1,data_train_x_2),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x_1 = data_signal.t()[0:6].t()\n",
    "data_train_x_2 = data_signal.t()[7:13].t()\n",
    "data_test_signal = torch.cat((data_train_x_1,data_train_x_2),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mu, z_var  = model.enc(data_test_bkg.float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = torch.exp(z_var / 2)\n",
    "eps = torch.randn_like(std)\n",
    "x_sample = eps.mul(std).add_(z_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_bkg = model.dec(x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bkg = torch.mean((decoded_bkg-data_test_bkg.float().cuda())**2,dim=1).data.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mu2, z_var2  = model.enc(data_test_signal.float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "std2 = torch.exp(z_var2 / 2)\n",
    "eps2 = torch.randn_like(std2)\n",
    "x_sample2 = eps2.mul(std2).add_(z_mu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sig = model.dec(x_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sig = torch.mean((decoded_sig-data_test_signal.float().cuda())**2,dim=1).data.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70918435, 2.4629786 , 0.56856245, ..., 0.5038268 , 0.66532624,\n",
       "       1.3086402 ], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_bkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0.,    8.,   26.,  118.,  279.,  554.,  797., 1077., 1423.,\n",
       "        1778., 2173., 2547., 2928., 3212., 3531., 3817., 4156., 4350.,\n",
       "        4413., 4449., 4499., 4571., 4276., 3996., 3733., 3518., 3249.,\n",
       "        2923., 2737., 2390., 2127., 1848., 1757., 1526., 1412., 1224.,\n",
       "        1106., 1047.,  878.,  842.,  713.,  649.,  607.,  572.,  510.,\n",
       "         444.,  391.,  417.,  353.,  317.,  282.,  266.,  236.,  250.,\n",
       "         191.,  159.,  153.,  163.,  124.,  124.,  109.,   98.,   78.,\n",
       "          96.,   92.,   72.,   88.,   73.,   57.,   72.,   48.,   47.,\n",
       "          53.,   47.,   36.,   41.,   37.,   41.,   36.,   29.,   25.,\n",
       "          31.,   26.,   33.,   31.,   25.,   18.,   15.,   15.,   12.,\n",
       "          17.,   16.,    7.,   20.,    9.,   11.,   15.,    8.,    7.]),\n",
       " array([0.        , 0.05050505, 0.1010101 , 0.15151515, 0.2020202 ,\n",
       "        0.25252525, 0.3030303 , 0.35353535, 0.4040404 , 0.45454545,\n",
       "        0.50505051, 0.55555556, 0.60606061, 0.65656566, 0.70707071,\n",
       "        0.75757576, 0.80808081, 0.85858586, 0.90909091, 0.95959596,\n",
       "        1.01010101, 1.06060606, 1.11111111, 1.16161616, 1.21212121,\n",
       "        1.26262626, 1.31313131, 1.36363636, 1.41414141, 1.46464646,\n",
       "        1.51515152, 1.56565657, 1.61616162, 1.66666667, 1.71717172,\n",
       "        1.76767677, 1.81818182, 1.86868687, 1.91919192, 1.96969697,\n",
       "        2.02020202, 2.07070707, 2.12121212, 2.17171717, 2.22222222,\n",
       "        2.27272727, 2.32323232, 2.37373737, 2.42424242, 2.47474747,\n",
       "        2.52525253, 2.57575758, 2.62626263, 2.67676768, 2.72727273,\n",
       "        2.77777778, 2.82828283, 2.87878788, 2.92929293, 2.97979798,\n",
       "        3.03030303, 3.08080808, 3.13131313, 3.18181818, 3.23232323,\n",
       "        3.28282828, 3.33333333, 3.38383838, 3.43434343, 3.48484848,\n",
       "        3.53535354, 3.58585859, 3.63636364, 3.68686869, 3.73737374,\n",
       "        3.78787879, 3.83838384, 3.88888889, 3.93939394, 3.98989899,\n",
       "        4.04040404, 4.09090909, 4.14141414, 4.19191919, 4.24242424,\n",
       "        4.29292929, 4.34343434, 4.39393939, 4.44444444, 4.49494949,\n",
       "        4.54545455, 4.5959596 , 4.64646465, 4.6969697 , 4.74747475,\n",
       "        4.7979798 , 4.84848485, 4.8989899 , 4.94949495, 5.        ]),\n",
       " <a list of 99 Patch objects>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0,5,100)\n",
    "plt.hist(loss_bkg,bins=bins,alpha=0.3,color='b',label='bkg')\n",
    "plt.hist(loss_sig,bins=bins,alpha=0.3,color='r',label='sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"BetaVAE_bkgae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(prefix+'_sigloss.npy',loss_sig)\n",
    "np.save(prefix+'_bkgloss.npy',loss_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_hdf(\"/data/t3home000/spark/LHCOlympics_previous/LHC-Olympics/Code/Nsubjettiness_mjj.h5\")\n",
    "dt = f.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = dt[:,15]\n",
    "bkg_idx = np.where(idx==0)[0]\n",
    "signal_idx = np.where(idx==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bkg = get_loss(dt[bkg_idx],[0,1,2,3,4,5,7,8,9,10,11,12])\n",
    "loss_sig = get_loss(dt[signal_idx],[0,1,2,3,4,5,7,8,9,10,11,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "bins = np.linspace(0,5,1000)\n",
    "plt.hist(loss_bkg,bins=bins,alpha=0.3,color='b',label='bkg')\n",
    "plt.hist(loss_sig,bins=bins,alpha=0.3,color='r',label='sig')\n",
    "plt.xlabel(r'Autoencoder Loss')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_fpr(sigloss,bkgloss,aetype='bkg'):\n",
    "    bins = np.linspace(0,50,1001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for cut in bins:\n",
    "        if aetype == 'sig':\n",
    "            tpr.append(np.where(sigloss<cut)[0].shape[0]/len(sigloss))\n",
    "            fpr.append(np.where(bkgloss<cut)[0].shape[0]/len(bkgloss))\n",
    "        if aetype == 'bkg':\n",
    "            tpr.append(np.where(sigloss>cut)[0].shape[0]/len(sigloss))\n",
    "            fpr.append(np.where(bkgloss>cut)[0].shape[0]/len(bkgloss))\n",
    "    return tpr,fpr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRECISION - RECALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall(sigloss,bkgloss,aetype='bkg'):\n",
    "    bins = np.linspace(0,100,1001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    precision = []\n",
    "    for cut in bins:\n",
    "        if aetype == 'sig':\n",
    "            tpr.append(np.where(sigloss<cut)[0].shape[0]/len(sigloss))\n",
    "            precision.append((np.where(sigloss<cut)[0].shape[0])/(np.where(bkgloss<cut)[0].shape[0]+np.where(sigloss<cut)[0].shape[0]))\n",
    "            \n",
    "        if aetype == 'bkg':\n",
    "            tpr.append(np.where(sigloss>cut)[0].shape[0]/len(sigloss))\n",
    "            precision.append((np.where(sigloss>cut)[0].shape[0])/(np.where(bkgloss>cut)[0].shape[0]+np.where(sigloss>cut)[0].shape[0]))\n",
    "    return precision,tpr      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
